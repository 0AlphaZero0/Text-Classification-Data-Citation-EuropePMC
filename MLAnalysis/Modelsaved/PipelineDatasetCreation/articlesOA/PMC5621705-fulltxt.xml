<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5621705</article-id><article-id pub-id-type="pmid">28926564</article-id><article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00432</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1005763</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Cell Biology</subject><subj-group><subject>Cellular Types</subject><subj-group><subject>Animal Cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular Neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Probability Distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Probability Density</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Computational Linguistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (Mathematics)</subject><subj-group><subject>Statistical Models</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Random Variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Probabilistic models for neural populations that naturally capture global coupling and criticality</article-title><alt-title alt-title-type="running-head">Probabilistic models for critical neural populations</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7573-7635</contrib-id><name><surname>Humplik</surname><given-names>Jan</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#8211; original draft</role><role content-type="http://credit.casrai.org/">Writing &#8211; review &amp; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6699-1455</contrib-id><name><surname>Tka&#269;ik</surname><given-names>Ga&#353;per</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#8211; original draft</role><role content-type="http://credit.casrai.org/">Writing &#8211; review &amp; editing</role><xref ref-type="aff" rid="aff001"/><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001">
<addr-line>Institute of Science and Technology Austria, Klosterneuburg, Austria</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Hennig</surname><given-names>Matthias H</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Edinburgh, UNITED KINGDOM</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>gtkacik@ist.ac.at</email></corresp></author-notes><pub-date pub-type="collection"><month>9</month><year>2017</year></pub-date><pub-date pub-type="epub"><day>19</day><month>9</month><year>2017</year></pub-date><volume>13</volume><issue>9</issue><elocation-id>e1005763</elocation-id><history><date date-type="received"><day>16</day><month>3</month><year>2017</year></date><date date-type="accepted"><day>5</day><month>9</month><year>2017</year></date></history><permissions><copyright-statement>&#169; 2017 Humplik, Tka&#269;ik</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Humplik, Tka&#269;ik</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pcbi.1005763.pdf"/><abstract><p>Advances in multi-unit recordings pave the way for statistical modeling of activity patterns in large neural populations. Recent studies have shown that the summed activity of all neurons strongly shapes the population response. A separate recent finding has been that neural populations also exhibit criticality, an anomalously large dynamic range for the probabilities of different population activity patterns. Motivated by these two observations, we introduce a class of probabilistic models which takes into account the prior knowledge that the neural population could be globally coupled and close to critical. These models consist of an energy function which parametrizes interactions between small groups of neurons, and an arbitrary positive, strictly increasing, and twice differentiable function which maps the energy of a population pattern to its probability. We show that: 1) augmenting a pairwise Ising model with a nonlinearity yields an accurate description of the activity of retinal ganglion cells which outperforms previous models based on the summed activity of neurons; 2) prior knowledge that the population is critical translates to prior expectations about the shape of the nonlinearity; 3) the nonlinearity admits an interpretation in terms of a continuous latent variable globally coupling the system whose distribution we can infer from data. Our method is independent of the underlying system&#8217;s state space; hence, it can be applied to other systems such as natural scenes or amino acid sequences of proteins which are also known to exhibit criticality.</p></abstract><abstract abstract-type="summary"><title>Author summary</title><p>Populations of sensory neurons represent information about the outside environment in a collective fashion. A salient property of this distributed neural code is criticality. Yet most models used to date to analyze recordings from large neural populations do not take this observation explicitly into account. Here we aim to bridge this gap by designing probabilistic models whose structure reflects the expectation that the population is close to critical. We show that such principled approach improves previously considered models, and we demonstrate a connection between our models and the presence of continuous latent variables which is a recently proposed mechanism underlying criticality in many natural systems.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002428</institution-id><institution>Austrian Science Fund</institution></institution-wrap></funding-source><award-id>P25651</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6699-1455</contrib-id><name><surname>Tkacik</surname><given-names>Gasper</given-names></name></principal-award-recipient></award-group><funding-statement>This work was supported in part by HFSP Program Grant RGP0065/2012 (<ext-link ext-link-type="uri" xlink:href="http://www.hfsp.org">www.hfsp.org</ext-link>) and Austrian Science Fund (FWF) grant P25651 (<ext-link ext-link-type="uri" xlink:href="http://www.fwf.ac.at">www.fwf.ac.at</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="0"/><page-count count="26"/></counts><custom-meta-group><custom-meta><meta-name>PLOS Publication Stage</meta-name><meta-value>vor-update-to-uncorrected-proof</meta-value></custom-meta><custom-meta><meta-name>Publication Update</meta-name><meta-value>2017-09-29</meta-value></custom-meta><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data can be downloaded from the IST Austria data repository at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.15479/AT:ISTA:61">https://doi.org/10.15479/AT:ISTA:61</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data can be downloaded from the IST Austria data repository at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.15479/AT:ISTA:61">https://doi.org/10.15479/AT:ISTA:61</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Recent progress in recording technology that permits monitoring the activity of large neural populations simultaneously has enabled us to infer detailed large-scale probabilistic models for neural activity and, hence, to document and interpret patterns of statistical dependencies between neural responses. Many questions regarding collective behavior in large populations of sensory neurons, previously in the domain of theoretical speculation, were thus brought into the spotlight and into contact with rich experimental data: How can large populations of sensory neurons encode information reliably despite the noise, and how can the stimulus information be recovered? How can downstream areas &#8220;learn&#8221; to read the neural code without direct access to the stimulus? Are there statistical features of the neural code that point to &#8220;design principles&#8221; at the population level and provide a prior over the space of possible neural codes? While stimulus-conditional (encoding) [<xref rid="pcbi.1005763.ref001" ref-type="bibr">1</xref>&#8211;<xref rid="pcbi.1005763.ref003" ref-type="bibr">3</xref>] and decoding approaches [<xref rid="pcbi.1005763.ref004" ref-type="bibr">4</xref>&#8211;<xref rid="pcbi.1005763.ref006" ref-type="bibr">6</xref>] have been instrumental for understanding stimulus representation, probabilistic models for the total distribution of neural activity [<xref rid="pcbi.1005763.ref007" ref-type="bibr">7</xref>] highlighted various salient statistical features of the neural code, two of which we focus on below.</p><p>The first salient feature is that neural populations are often &#8220;globally coupled.&#8221; While it has been appreciated for some time that neurons do not spike independently, the approximation that their interactions are well-described by low-order statistical dependencies (e.g., pairwise interactions) has provided powerful descriptions of the data, known as pairwise maximum entropy (Ising-like) models or, alternatively, as fully-visible Boltzmann machines [<xref rid="pcbi.1005763.ref008" ref-type="bibr">8</xref>&#8211;<xref rid="pcbi.1005763.ref010" ref-type="bibr">10</xref>]. As the recorded populations grew to tens or hundreds of neurons, however, it became increasingly clear that pairwise models are insufficient [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>]. Instead of increasing model complexity order-by-order (e.g., by including triplet interactions [<xref rid="pcbi.1005763.ref012" ref-type="bibr">12</xref>]) which quickly becomes intractable, one proposal has been to directly identify global or collective modes of activity and build models that reproduce them well. In the retina, for example, the population synchrony, or the summed activity over all neurons in a given time bin, represents one such global statistic that probabilistic models can reproduce, leading to the so-called &#8220;K-pairwise&#8221; models [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1005763.ref013" ref-type="bibr">13</xref>, <xref rid="pcbi.1005763.ref014" ref-type="bibr">14</xref>]. Similar ideas relate to models able to capture the no-spike probability in groups of neurons in hippocampal slices [<xref rid="pcbi.1005763.ref015" ref-type="bibr">15</xref>], or correlation between population synchrony and firing of individual neurons in the cortex [<xref rid="pcbi.1005763.ref016" ref-type="bibr">16</xref>&#8211;<xref rid="pcbi.1005763.ref018" ref-type="bibr">18</xref>]. In all cases, the increased performance of the proposed models originates in the models&#8217; ability to capture higher-order correlations in neural spiking through a smart guess for the global (macroscopic) statistic of the population activity.</p><p>The second salient feature is that neural population responses are close to critical in a thermodynamic sense [<xref rid="pcbi.1005763.ref010" ref-type="bibr">10</xref>, <xref rid="pcbi.1005763.ref019" ref-type="bibr">19</xref>]. This criticality is distinct from the dynamical, avalanche-type criticality that has been studied extensively in the past [<xref rid="pcbi.1005763.ref020" ref-type="bibr">20</xref>, <xref rid="pcbi.1005763.ref021" ref-type="bibr">21</xref>], although formal connections between the two notions may exist [<xref rid="pcbi.1005763.ref022" ref-type="bibr">22</xref>]. We give a precise definition of thermodynamic criticality below. Intuitively and informally, criticality of the ensemble of patterns of spikes and silences implies the following: (i) the distribution of neural responses is Zipfian, with a slope of &#8722;1 on a log-frequency vs log-rank plot; equivalently, the (log) density of states and (log) probability of responses are linearly related [<xref rid="pcbi.1005763.ref019" ref-type="bibr">19</xref>, <xref rid="pcbi.1005763.ref023" ref-type="bibr">23</xref>]; (ii) the dynamic range of neural response probabilities is anomalously large in a certain mathematical sense [<xref rid="pcbi.1005763.ref010" ref-type="bibr">10</xref>]; (iii) there is no clearly definable information-theoretic &#8220;typical set&#8221; of responses; (iv) even though responses are of high dimensionality, one is likely to observe certain patterns of spiking and silence multiple times in a typical experiment [<xref rid="pcbi.1005763.ref014" ref-type="bibr">14</xref>, <xref rid="pcbi.1005763.ref024" ref-type="bibr">24</xref>].</p><p>Several works pursued the origins of the observed signatures of criticality [<xref rid="pcbi.1005763.ref025" ref-type="bibr">25</xref>&#8211;<xref rid="pcbi.1005763.ref030" ref-type="bibr">30</xref>]. Two recent papers [<xref rid="pcbi.1005763.ref028" ref-type="bibr">28</xref>, <xref rid="pcbi.1005763.ref029" ref-type="bibr">29</xref>] focused on the role of unobserved (latent) variables whose fluctuations, coupled to the observable responses of individual neurons, could lead to critical response ensemble under relatively generic conditions. While these works provided an interesting proof-of-concept analysis, it has remained unclear whether these ideas could be incorporated into a probabilistic model that could be tractably inferred from large-scale data and that would simultaneously recapitulate the critical behavior through the proposed mechanism, match in detail the many previously documented statistical features of the neural code, and provide cross-validated performance similar to existing state-of-the-art models.</p><p>Here we introduce a new class of probabilistic models for the neural code called <italic>semiparametric energy-based models</italic>. These models explicitly incorporate our prior belief that the neural population could be globally coupled and close to critical. If data indeed exhibits such features, our models can capture them efficiently; otherwise, our models can reduce to previously studied energy-based (e.g., pairwise maximum entropy) models. We infer our models on populations of 100+ retinal ganglion cells and show that they provide superior performance over K-pairwise models. We further show that our models capture critical behavior by a mechanism that is mathematically equivalent to the fluctuating latent variable model, and give an interpretation of the resulting latent variable as defining the state of the retinal population to be &#8220;active&#8221; or &#8220;silent.&#8221; Importantly, the central idea of the framework introduced here extends beyond the neural code in general and the retina in particular: any energy-based probabilistic model can be augmented with our proposed mechanism. This flexibility is relevant since other interesting datasets, such as natural image patches [<xref rid="pcbi.1005763.ref031" ref-type="bibr">31</xref>, <xref rid="pcbi.1005763.ref032" ref-type="bibr">32</xref>] or certain genomic sequences [<xref rid="pcbi.1005763.ref033" ref-type="bibr">33</xref>], also exhibit critical and globally coupled nature.</p><sec id="sec002"><title>Models of globally coupled neural populations</title><p>We represent the response of a neural population with a binary vector <bold>s</bold> = {<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>, &#8230;, <italic>s</italic><sub><italic>N</italic></sub>} &#8712; {0, 1}<sup><italic>N</italic></sup> identifying which of the <italic>N</italic> neurons elicited at least one action potential (&#8216;1&#8217;) and which stayed silent (&#8216;0&#8217;) during a short time window. Our goal is to build a model for the probability distribution of activity patterns, <italic>p</italic>(<bold>s</bold>), given a limited number <italic>M</italic> of samples, <inline-formula id="pcbi.1005763.e001"><alternatives><graphic xlink:href="pcbi.1005763.e001.jpg" id="pcbi.1005763.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, observed in a typical recording session. The regime we are mainly interested in is the one where the dimensionality of the problem is sufficiently high that the distribution <italic>p</italic> cannot be directly sampled from data, i.e., when 2<sup><italic>N</italic></sup> &#8811; <italic>M</italic>. Note that we are looking to infer models for the unconditional distribution over neural activity patterns (i.e., the population &#8220;vocabulary&#8221;), explored in a number of recent papers [<xref rid="pcbi.1005763.ref008" ref-type="bibr">8</xref>, <xref rid="pcbi.1005763.ref009" ref-type="bibr">9</xref>, <xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1005763.ref013" ref-type="bibr">13</xref>&#8211;<xref rid="pcbi.1005763.ref018" ref-type="bibr">18</xref>, <xref rid="pcbi.1005763.ref024" ref-type="bibr">24</xref>, <xref rid="pcbi.1005763.ref034" ref-type="bibr">34</xref>], rather than to construct stimulus-conditional models (i.e., the &#8220;encoding models&#8221;, which have a long tradition in computational neuroscience [<xref rid="pcbi.1005763.ref001" ref-type="bibr">1</xref>&#8211;<xref rid="pcbi.1005763.ref003" ref-type="bibr">3</xref>]).</p><p>Previous approaches to modeling globally coupled populations focused on the total network activity, also known as synchrony, <inline-formula id="pcbi.1005763.e002"><alternatives><graphic xlink:href="pcbi.1005763.e002.jpg" id="pcbi.1005763.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. The importance of this quantity was first analyzed in the context of probabilistic models in Ref [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>] where the authors showed that a K-pairwise model, which generalizes a pairwise maximum entropy model by placing constraints on the statistics of <italic>K</italic>(<bold>s</bold>), is much better at explaining the observed population responses of 100+ salamander retinal ganglion cells than a pairwise model. Specifically, a pairwise model assumes that the covariance matrix between single neuron responses, <italic>C</italic><sub><italic>ij</italic></sub> = &#9001;<italic>s</italic><sub><italic>i</italic></sub><italic>s</italic><sub><italic>j</italic></sub>&#9002;, which can be determined empirically from data <inline-formula id="pcbi.1005763.e003"><alternatives><graphic xlink:href="pcbi.1005763.e003.jpg" id="pcbi.1005763.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula>, is sufficient to estimate the probability of any population activity pattern. In the maximum entropy framework, this probability is given by the most unstructured (or random) distribution that reproduces exactly the measured <italic>C</italic><sub><italic>ij</italic></sub>:
<disp-formula id="pcbi.1005763.e004"><alternatives><graphic xlink:href="pcbi.1005763.e004.jpg" id="pcbi.1005763.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>Z</italic>(<bold><italic>J</italic></bold>) is a normalization constant, and <bold><italic>J</italic></bold> is a coupling matrix which is chosen so that samples from the model have the same covariance matrix as data. Note that because <inline-formula id="pcbi.1005763.e005"><alternatives><graphic xlink:href="pcbi.1005763.e005.jpg" id="pcbi.1005763.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, the diagonal terms <italic>J</italic><sub><italic>ii</italic></sub> of the coupling matrix correspond to single neuron biases, i.e. firing probabilities in the absence of spikes from other neurons (previous work [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>] used a representation <italic>s</italic><sub><italic>i</italic></sub> &#8712; {&#8722;1, 1} for which the single neuron biases need to be included as separate parameters and where <italic>J</italic><sub><italic>ii</italic></sub> are all 0). A K-pairwise model generalizes the pairwise model and has the form
<disp-formula id="pcbi.1005763.e006"><alternatives><graphic xlink:href="pcbi.1005763.e006.jpg" id="pcbi.1005763.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>,</mml:mo><mml:mi>&#981;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>,</mml:mo><mml:mi>&#981;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#981;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
The coupling matrix <bold><italic>J</italic></bold> has the same role as in a pairwise model while the additional parameters <italic>&#981;</italic> are chosen to match the probability distribution of <italic>K</italic>(<bold>s</bold>) under the model to that estimated from data. The &#8220;potentials&#8221; <italic>&#981;</italic><sub><italic>k</italic></sub> introduced into the K-pairwise probabilistic model, <xref ref-type="disp-formula" rid="pcbi.1005763.e006">Eq (2)</xref>, globally couple the population, and cannot be reduced to low-order interactions between, e.g., pairs or triplets, of neurons, except in very special cases. We will generically refer to probabilistic models that impose non-trivial constraints on population-level statistics (of which the distribution of total network activity <italic>K</italic> is one particular example) as &#8220;globally coupled&#8221; models.</p><p>Here we introduce new <italic>semiparametric energy-based models</italic> that extend the notion of global coupling. These models are defined as follows:
<disp-formula id="pcbi.1005763.e007"><alternatives><graphic xlink:href="pcbi.1005763.e007.jpg" id="pcbi.1005763.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
where <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) is some <italic>energy function</italic> parametrized by <bold><italic>&#945;</italic></bold>, and <italic>V</italic> is an arbitrary increasing differentiable function which we will refer to simply as the &#8220;nonlinearity.&#8221; The parametrization of the energy function should be chosen so as to reflect local interactions among neurons. Crucially, while it is necessary to choose a specific parametrization of the energy function, we do not make any assumptions on the shape of the nonlinearity&#8212;we let the shape be determined nonparametrically from data. <xref ref-type="fig" rid="pcbi.1005763.g001">Fig 1</xref> schematically displays the relationship between the previously studied probabilistic models of population activity and two semiparametric energy-based models that we focus on in this paper, the <italic>semiparametric independent model</italic> (which we also refer to as &#8220;V(independent)&#8221;) and the <italic>semiparametric pairwise model</italic> (which we also refer to as &#8220;V(pairwise)&#8221;).</p><fig id="pcbi.1005763.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g001</object-id><label>Fig 1</label><caption><title>Overview of models which contain mechanisms for capturing global coupling.</title><p>At any given time, the population activity pattern is defined by neurons which either spike (<italic>s</italic><sub><italic>i</italic></sub> = 1, dark discs) or are silent (<italic>s</italic><sub><italic>i</italic></sub> = 0, white discs). The probability of spiking is partially determined by an intrinsic firing bias (<italic>&#945;</italic><sub><italic>i</italic></sub> for models without local interactions, or the diagonal terms of the coupling matrix <italic>J</italic> for models with local pairwise interactions). When local interactions between neurons are important, they can be parametrized by assigning each pair of neurons a coupling weight. Positive weight (orange) increases the likelihood of the paired neurons spiking together, while negative weight (blue) decreases the likelihood. The negative sum of the intrinsic firing biases of active neurons and the coupling weights of pairs which fire synchronously is referred to as the energy of the population activity pattern. The probability of a given pattern is simply proportional to the exponential of its negative energy. To capture correlations due to global coupling, previous studies considered models which bias the response probabilities with a function of the total network activity (here denoted as <italic>K</italic>, i.e., the sum of the activities of individual neurons). We introduce a different approach (shaded models in the figure) where global coupling is induced by mapping the energy of the activity pattern to its probability with an arbitrary (smooth and increasing) function exp(&#8722;<italic>V</italic> (<italic>E</italic>)).</p></caption><graphic xlink:href="pcbi.1005763.g001"/></fig><p>Our motivation for introducing the global coupling via the nonlinearity <italic>V</italic> traces back to the argument made in Ref [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>] for choosing to constrain the statistics of synchrony, <italic>K</italic>(<bold>s</bold>); in short, the key intuition in earlier work has been that <italic>K</italic>(<bold>s</bold>) is a biologically relevant quantity which encodes information about the global state of a population. There are, however, many other quantities whose distributions could contain signatures of global coupling in a population. In particular, while most energy functions&#8212;e.g., the pairwise energy function, <italic>E</italic>(<bold>s</bold>; <bold><italic>J</italic></bold>) = &#8722;&#8721;<sub><italic>i</italic>,<italic>j</italic></sub>
<italic>J</italic><sub><italic>ij</italic></sub><italic>s</italic><sub><italic>i</italic></sub><italic>s</italic><sub><italic>j</italic></sub>&#8212;are defined solely in terms of local interactions between small groups of neurons, the statistics of these same energy functions (for instance, their moments) are strongly shaped by global effects. Specifically, we show in Methods that the role of the nonlinearity in <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> is precisely to match the probability density of the energy under the model to that estimated from data. In other words, once any energy function for <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> has been chosen, the nonlinearity <italic>V</italic> will ensure that the distributions of that particular energy in the model and over data samples agree.</p><p>Constraining the statistics of the energy <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) is different from constraining the statistics of <italic>K</italic>(<bold>s</bold>), used in previous work. First, the energy depends on a priori unknown parameters <bold><italic>&#945;</italic></bold> which must be learned from data. Second, while <italic>K</italic>(<bold>s</bold>) is always an integer between 0 and <italic>N</italic>, the energy can take up to 2<sup><italic>N</italic></sup> distinct values; this allows for extra richness but also requires us to constrain the (smoothed) histogram of energy rather than the probability of every possible energy value, to prevent overfitting.</p><p>As we discuss next, the statistics of the energy are also closely related to criticality, a formal, model-free property distinguishing large, globally-coupled neural populations.</p></sec><sec id="sec003"><title>Criticality</title><p>The notion of criticality originates in thermodynamics where it encompasses several different properties of systems undergoing a second-order phase transition [<xref rid="pcbi.1005763.ref035" ref-type="bibr">35</xref>]. Today, many other phenomena, such as power-law distributed sizes of &#8220;avalanches&#8221; in neural activity, have been termed critical [<xref rid="pcbi.1005763.ref020" ref-type="bibr">20</xref>]. Our definition, which we discuss below, is a restricted version of the thermodynamic criticality.</p><p>We consider a sequence of probability distributions <inline-formula id="pcbi.1005763.e008"><alternatives><graphic xlink:href="pcbi.1005763.e008.jpg" id="pcbi.1005763.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>&#8734;</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> over the responses of neural populations of increasing sizes, <italic>N</italic>. These probability distributions define the discrete random variable <bold>s</bold> (the population response), but they can also be thought of simply as functions which map a population response to a number between 0 and 1. Combining these two viewpoints, we can consider a real-valued random variable <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>) &#8712; (0, 1) which is constructed by applying the function <italic>p</italic><sub><italic>N</italic></sub> to the random variable <bold>s</bold>. The behavior of this random variable as <italic>N</italic> &#8594; &#8734; is often universal, meaning that some of its features are independent of the precise form of <italic>p</italic><sub><italic>N</italic></sub>. As is conventional, we work with the logarithm of <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>) instead of the actual distribution. We call a population &#8220;critical&#8221; if the standard deviation of the random variable log <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>)/<italic>N</italic> does not vanish as the population size becomes large, i.e.
<disp-formula id="pcbi.1005763.e009"><alternatives><graphic xlink:href="pcbi.1005763.e009.jpg" id="pcbi.1005763.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>&#963;</mml:mi><mml:mspace width="1pt"/><mml:mo stretchy="false">(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8603;</mml:mo><mml:mspace width="1.pt"/><mml:mn>0</mml:mn><mml:mspace width="4.pt"/><mml:mtext>as</mml:mtext><mml:mspace width="4.pt"/><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>&#8734;</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
(For completeness, we further exclude some degenerate cases such as when the probability density of log <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>)/<italic>N</italic> converges to two equally sized delta functions.)</p><p>The above definition is related to criticality as studied in statistical physics. In thermodynamics, <inline-formula id="pcbi.1005763.e010"><alternatives><graphic xlink:href="pcbi.1005763.e010.jpg" id="pcbi.1005763.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mi>&#963;</mml:mi><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> is proportional to the square root of the specific heat, which diverges in systems undergoing a second-order phase transition. While at a thermodynamical critical point <italic>&#963;</italic> (log <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>))/<italic>N</italic> scales as <italic>N</italic><sup>&#8722;<italic>&#947;</italic></sup> with <italic>&#947;</italic> &#8712; (0, 1/2), here we are concerned with the extreme case of <italic>&#947;</italic> = 0. Rather than being related to second-order phase transitions, this definition of criticality is related to the so-called Zipf law [<xref rid="pcbi.1005763.ref023" ref-type="bibr">23</xref>].</p><p>A pattern <bold>s</bold> can be assigned a rank by counting how many other patterns have a higher probability. In its original form, a probability distribution is said to satisfy Zipf law if the probability of a pattern is inversely proportional to its rank. No real probability distribution is actually expected to satisfy this definition precisely, but there is a weaker form of Zipf law which concerns very large populations, and which is much less restrictive. This weaker form can be stated as a smoothed version of the original Zipf law. Consider patterns whose rank is in some small interval [<italic>r</italic>, <italic>r</italic> + &#916;<sub><italic>N</italic></sub>], and denote <italic>p</italic><sub><italic>N</italic></sub>(<italic>r</italic>) the average probability of these patterns. We generalize the notion of Zipf law to mean that for very large populations <italic>p</italic><sub><italic>N</italic></sub>(<italic>r</italic>) &#8733; <italic>r</italic><sup>&#8722;1</sup> (&#916;<sub><italic>N</italic></sub> is assumed to go to zero sufficiently quickly with <italic>N</italic>). As shown in Ref [<xref rid="pcbi.1005763.ref023" ref-type="bibr">23</xref>], a system is critical in the sense of <xref ref-type="disp-formula" rid="pcbi.1005763.e009">Eq (4)</xref> precisely when it follows this generalized Zipf law. Practically speaking, no experimentally studied population ever has an infinite size, and a typical way to check for signs of criticality is to see if a log-log plot of a pattern probability versus its rank resembles a straight line with slope &#8722;1.</p><p>Most systems are not expected to be critical. The simplest example is a population of identical and independent neurons,
<disp-formula id="pcbi.1005763.e011"><alternatives><graphic xlink:href="pcbi.1005763.e011.jpg" id="pcbi.1005763.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic>q</italic> is the probability of eliciting a spike. For such population,
<disp-formula id="pcbi.1005763.e012"><alternatives><graphic xlink:href="pcbi.1005763.e012.jpg" id="pcbi.1005763.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>&#963;</mml:mi><mml:mspace width="1pt"/><mml:mo stretchy="false">(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msqrt><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo form="prefix">log</mml:mo><mml:mfrac><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
which vanishes for very large number of neurons, and so the system is not critical. More generally, if <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>) can be factorized into a product of probability distributions over smaller subpopulations which are independent of each other and whose number is proportional to <italic>N</italic>, then log <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>)/<italic>N</italic> turns into an empirical average whose standard deviation is expected to vanish in the large <italic>N</italic> limit, and the population is not critical. Reversing this argument, signatures of criticality can be interpreted as evidence that the population is globally coupled, i.e. that it cannot be decomposed into independent parts.</p><p>These preliminaries establish a direct link between criticality and semiparametric energy models of <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref>. Nonlinearity in semiparametric energy models makes sure that the statistics of the energy <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>), and, since <italic>V</italic> (<italic>E</italic>) is monotone, also the statistics of log <italic>p</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>, <italic>V</italic>) are modeled accurately (see <xref ref-type="sec" rid="sec011">Methods</xref>). Because the behavior of log probability is crucial for criticality, as argued above, semiparametric energy models can capture accurately and efficiently the relevant statistical structure of any system that exhibits signs of criticality and/or global coupling.</p></sec><sec id="sec004"><title>Nonparametric estimation of the nonlinearity</title><p>To fully specify semiparametric energy models, we need a procedure for constructing the nonlinearity <italic>V</italic> (<italic>E</italic>). We cannot let this function be arbitrary because then the model could learn to assign nonzero probabilities only to the samples in the dataset, and hence it would overfit. To avoid such scenarios, we will restrict ourselves to functions which are increasing. We also require <italic>V</italic> (<italic>E</italic>) to be differentiable so that we can utilize its derivatives when fitting the model to data. The class of increasing differentiable functions is very large. It includes functions as diverse as the sigmoid, 1/(1 + exp(&#8722;<italic>E</italic>)), and the square root, <inline-formula id="pcbi.1005763.e013"><alternatives><graphic xlink:href="pcbi.1005763.e013.jpg" id="pcbi.1005763.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:msqrt><mml:mi>E</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula> (for positive <italic>E</italic>), but we do not want to restrict ourselves to any such particular form&#8212;we want to estimate <italic>V</italic> (<italic>E</italic>) nonparametrically.</p><p>Nonparametric estimation of monotone differentiable functions is a nontrivial yet very useful task (for example, consider tracking the height of a child over time&#8212;the child is highly unlikely to shrink at any given time). We follow Ref [<xref rid="pcbi.1005763.ref036" ref-type="bibr">36</xref>] and restrict ourselves to the class of strictly monotone twice differentiable functions for which <italic>V</italic>&#8242;&#8242;/<italic>V</italic>&#8242; is square-integrable. Any such function can be represented in terms of a square-integrable function <italic>W</italic> and two constants <italic>&#947;</italic><sub>1</sub> and <italic>&#947;</italic><sub>2</sub> as
<disp-formula id="pcbi.1005763.e014"><alternatives><graphic xlink:href="pcbi.1005763.e014.jpg" id="pcbi.1005763.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="1pt"/><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mi>E</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mspace width="1pt"/><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msubsup><mml:mspace width="1pt"/><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
where <italic>E</italic><sub>0</sub> is arbitrary and sets the constants to <italic>&#947;</italic><sub>1</sub> = <italic>V</italic> (<italic>E</italic><sub>0</sub>), <italic>&#947;</italic><sub>2</sub> = <italic>V</italic>&#8242;(<italic>E</italic><sub>0</sub>). The function is either everywhere increasing or everywhere decreasing (depending on the sign of <italic>&#947;</italic><sub>2</sub>) because the exponential is always positive. <xref ref-type="disp-formula" rid="pcbi.1005763.e014">Eq (7)</xref> is easier to understand by noting that <italic>V</italic> (<italic>E</italic>) is a solution to the differential equation <italic>V</italic>&#8242;&#8242; = <italic>WV</italic>&#8242;. This means, for example, that on any interval on which <italic>W</italic> = 0, the equation reduces to <italic>V</italic>&#8242;&#8242; = 0, and so <italic>V</italic> (<italic>E</italic>) is a linear function on this interval. If <italic>V</italic> (<italic>E</italic>) is increasing (<italic>V</italic>&#8242; &gt; 0), it also shows that the sign of <italic>W</italic> at a given point determines the sign of the second derivative of <italic>V</italic> at that point.</p><p>An advantage of writing the nonlinearity in the form of <xref ref-type="disp-formula" rid="pcbi.1005763.e014">Eq (7)</xref> is that we can parametrize it by expanding <italic>W</italic> in an arbitrary basis without imposing any constraints on the coefficients of the basis vectors yet <italic>V</italic> (<italic>E</italic>) is still guaranteed to be monotone and smooth. In particular, we will use piecewise-constant functions for W. This allows us to use unconstrained optimization techniques for fitting our models to data.</p></sec></sec><sec sec-type="results" id="sec005"><title>Results</title><p>We analyzed a simultaneous recording from 160 neurons in a salamander retina which was presented with 297 repetitions of a 19 second natural movie. The data was collected as part of a previous study [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>], and is publicly available [<xref rid="pcbi.1005763.ref037" ref-type="bibr">37</xref>]. All models were trained using a variation of Persistent Contrastive Divergence [<xref rid="pcbi.1005763.ref038" ref-type="bibr">38</xref>] which performs an approximate gradient ascent on the log-likelihood of data. The nonparametric estimate of <italic>V</italic> only added 20 additional parameters to each model, and the gradient ascent learned these parameters simultaneously with the parameters of the energy function. Details regarding the parametrization of <italic>V</italic> and the algorithm for learning models from data can be found in Methods, and our code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jhumplik/generative-neural-models">https://github.com/jhumplik/generative-neural-models</ext-link>.</p><p>The population responses were binary vectors <bold>s</bold> &#8712; {0, 1}<sup><italic>N</italic></sup> representing which neurons elicited an action potential during a 20 ms time window. All responses were pooled across time and repeats; hence, we did not utilize the repeat structure in any way during model inference. For some analyses we examined the scaling of various quantities of interest with the population size. To this end, we used our data to construct 30 smaller datasets as follows. We randomly select 40 neurons from the total of 160 as the first dataset. Then we augment this dataset with 20 additional neurons to yield the second dataset, and we keep repeating this process until we have a dataset of 140 neurons. This whole process is repeated 5 times, resulting in 5 datasets for each of the 6 different population sizes. For each dataset, we set aside responses corresponding to randomly selected 60 (out of 297) repetitions of the movie, and use these as test data.</p><sec id="sec006"><title>Semiparametric independent model</title><p>We start by considering one of the simplest models of the form <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref>, the <italic>semiparametric independent model</italic>:
<disp-formula id="pcbi.1005763.e015"><alternatives><graphic xlink:href="pcbi.1005763.e015.jpg" id="pcbi.1005763.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula>
If <italic>V</italic> were a linear function, the model would reduce to an <italic>independent model</italic>, i.e. a population of independent neurons with diverse firing rates. In general, however, <italic>V</italic> introduces interactions between the neurons that may not have a straightforward low-order representation. When fitted to our data, the nonlinearity <italic>V</italic> turns out to be a concave function (see later sections on more complex models for a detailed discussion of the shape of the nonlinearity). Note that if <italic>V</italic> had a simple functional form such as a low order polynomial, then the model <xref ref-type="disp-formula" rid="pcbi.1005763.e015">Eq (8)</xref> would be closely related to mean field models of ferromagnetism with heterogenous local magnetic field studied in physics.</p><p>Our first goal is to use this simple model to verify our intuition that the nonlinearity helps to capture criticality. Many population patterns are observed several times during the course of the experiment, and so it is possible to estimate their probability simply by counting how often they occur in the data [<xref rid="pcbi.1005763.ref019" ref-type="bibr">19</xref>]. Given this empirical distribution, we construct a corresponding Zipf plot&#8212;a scatter plot of the frequency of a pattern vs its rank. For systems which are close to critical, this should yield a straight line with slope close to &#8722;1 on a log-log scale. We repeat the same procedure with samples generated from a semiparametric independent model as well as an independent model, which were both fitted to the responses of all 160 neurons. <xref ref-type="fig" rid="pcbi.1005763.g002">Fig 2</xref> shows all three scatter plots. The independent model vastly deviates from the empirical Zipf plot; specifically, it greatly underestimates the probabilities of the most likely states. In contrast, the learned semiparametric independent model follows a similar trend to that observed in data. This does not mean that the semiparametric independent model itself is an excellent model for the detailed structure in the data, but it is one of the simplest possible extensions of the trivial independent model that qualitatively captures both global coupling and the signatures of criticality.</p><fig id="pcbi.1005763.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g002</object-id><label>Fig 2</label><caption><title>Semiparametric independent model reproduces the empirical Zipf plot.</title><p>Each curve shows the probabilities of population activity patterns, <italic>P</italic>(<bold>s</bold>), sorted in decreasing order on a log-log plot. To construct the empirical Zipf plot, we directly sampled the frequencies of different patterns from data. To construct model predictions, we used the same procedure but replaced real data with artificial datasets of the same size, generated by drawing the samples from the corresponding model. Error bars are 3 SD (bootstrapped).</p></caption><graphic xlink:href="pcbi.1005763.g002"/></fig><p>Since the semiparametric independent model is able to capture the criticality of the data distribution, we also expect it to accurately model other features of the data which are related to the globally coupled nature of the population. To verify this, <xref ref-type="fig" rid="pcbi.1005763.g003">Fig 3A</xref> compares the empirical probability distribution of the total activity of the population <italic>K</italic>(<bold>s</bold>) = &#8721;<sub><italic>i</italic></sub>
<italic>s</italic><sub><italic>i</italic></sub> to that predicted by the semiparametric independent model. The match is very accurate, especially when compared to the same distribution predicted by the independent model. This result goes hand in hand with the analysis in [<xref rid="pcbi.1005763.ref039" ref-type="bibr">39</xref>] which showed that interactions of all orders (in our case mediated by the nonlinearity) are necessary to model the wide-spread distribution of the total activity.</p><fig id="pcbi.1005763.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g003</object-id><label>Fig 3</label><caption><title>Comparison of the semiparametric independent and the independent model.</title><p><bold>A)</bold> Probability distributions of the total activity of the population, <italic>K</italic>(<bold>s</bold>) = &#8721;<sub><italic>i</italic></sub>
<italic>s</italic><sub><italic>i</italic></sub>, estimated from data and from model samples. Error bars are 3 SD (bootstrapped), with the model-generated sample size equal to that of the data. <bold>B)</bold> Comparison of the firing rates estimated from the data and from the model samples. The firing rates predicted by the independent model should exactly match the true firing rates. Error bars are 3 SD (bootstrapped). <bold>C)</bold> Comparison of the predicted pairwise covariance matrix elements estimated from the model and from data, for the semiparametric independent and the independent models. The scatter of independent model covariance elements around 0 illustrates the magnitude of the sampling noise.</p></caption><graphic xlink:href="pcbi.1005763.g003"/></fig><p>The independent model is a maximum entropy model which constrains the mean responses, &#9001;<italic>s</italic><sub><italic>i</italic></sub>&#9002;, of all neurons. In other words, neurons sampled from the model would have the same firing rates as those in the data (up to sampling noise). Even though the semiparametric independent model is strictly more general, it does not retain this property when the parameters <bold><italic>&#945;</italic></bold> and the nonlinearity <italic>V</italic> are learned by maximizing the likelihood of data. <xref ref-type="fig" rid="pcbi.1005763.g003">Fig 3B</xref> demonstrates this point: although the predicted firing rates are approximately correct, there are slight deviations. On the other hand, the nonlinearity induces pairwise correlations between neurons which is something the independent model by construction cannot do. <xref ref-type="fig" rid="pcbi.1005763.g003">Fig 3C</xref> compares these predicted pairwise correlations to their data estimates. While there is some correlation between the predicted and observed covariances, the semiparametric independent model often underestimates the magnitude of the covariances and does not capture the fine details of their structure (e.g. the largest covariance predicted by the semiparametric independent model is about 5&#215; smaller than the largest covariance observed in the data). This is because a combination of independent terms and a single nonlinearity does not have sufficient expressive power, motivating us to look for a richer model.</p></sec><sec id="sec007"><title>Semiparametric pairwise model</title><p>One way to augment the power of the semiparametric independent model that permits a clear comparison to previous work is by means of the semiparametric pairwise model:
<disp-formula id="pcbi.1005763.e016"><alternatives><graphic xlink:href="pcbi.1005763.e016.jpg" id="pcbi.1005763.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
We fit this model to the responses of the various subpopulations of the 160 neurons, and we compare the resulting goodness-of-fit to that of a pairwise (<xref ref-type="disp-formula" rid="pcbi.1005763.e004">Eq (1)</xref>), K-pairwise (<xref ref-type="disp-formula" rid="pcbi.1005763.e006">Eq (2)</xref>), and semiparametric independent model (<xref ref-type="disp-formula" rid="pcbi.1005763.e015">Eq (8)</xref>). We measure goodness-of-fit as the improvement of the log-likelihood of data per neuron under the model relative to the pairwise model, as shown in <xref ref-type="fig" rid="pcbi.1005763.g004">Fig 4A</xref>. This measure reflects differences among models rather than differences among various subpopulations. The semiparametric pairwise model consistently outperforms the other models and this difference grows with the population size. To make sure that this improvement is not specific to this particular experiment, we also fitted the models to two additional recordings from the salamander retina which were also collected as part of the study [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>]. One consists of 120 neurons responding to 69 repeats of a 30 second random checkerboard stimulus, and the other of 111 neurons responding to 98 repeats of a 10 second random full-field flicker stimulus. As shown in <xref ref-type="fig" rid="pcbi.1005763.g004">Fig 4B</xref>, the improvements of individual models on these datasets are consistent with the ones observed for the population stimulated with a natural movie.</p><fig id="pcbi.1005763.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g004</object-id><label>Fig 4</label><caption><title>Semiparametric pairwise model outperforms other models.</title><p><bold>A)</bold> Out-of-sample log-likelihood improvement relative to the pairwise model per sample per neuron averaged over subnetworks. Error bars denote variation over subnetworks (1 SD, no errorbars for <italic>N</italic> = 160 since there is only one subpopulation of that size in the entire dataset). The error in likelihood estimation is much smaller than the displayed error bars. <bold>B)</bold> The same as in A) but for single populations from two different experiments&#8211;one in which the population is stimulated with a random checkerboard stimulus, and the other where the population responds to a full-field flickering. <bold>C)</bold> The test set error rate averaged over neurons for predicting the response of a neuron from the activities of other neurons in 5 different subpopulations of 100 neurons. <bold>D)</bold> Average (across neurons) error rate decrease achieved by using a semiparametric pairwise model instead of a K-pairwise model for subpopulations of various sizes. Error bars denote 1 SD variation over subnetworks.</p></caption><graphic xlink:href="pcbi.1005763.g004"/></fig><p>The advantage of using likelihood as a goodness-of-fit measure is its universal applicability which, however, comes hand-in-hand with the difficulty of interpreting the quantitative likelihood differences between various models. An alternative comparison measure that has more direct relevance to neuroscience asks about how well the activity of a single chosen neuron can be predicted from the activities of other neurons in the population. Given any probabilistic model for the population response, we use Bayes rule to calculate the probability of the <italic>i</italic>th neuron spiking (<italic>s</italic><sub><italic>i</italic></sub> = 1) or being silent (<italic>s</italic><sub><italic>i</italic></sub> = 0) conditioned on the activity of the rest of the population (<bold>s</bold><sub>&#8722;<italic>i</italic></sub>) as
<disp-formula id="pcbi.1005763.e017"><alternatives><graphic xlink:href="pcbi.1005763.e017.jpg" id="pcbi.1005763.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(10)</label></disp-formula>
We turn this probabilistic prediction into a nonrandom one by choosing whether the neuron is more likely to spike or be silent given the rest of the population, i.e.
<disp-formula id="pcbi.1005763.e018"><alternatives><graphic xlink:href="pcbi.1005763.e018.jpg" id="pcbi.1005763.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">argmax</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(11)</label></disp-formula>
In <xref ref-type="fig" rid="pcbi.1005763.g004">Fig 4C and 4D</xref> we compare such predictive single neuron models constructed from semiparametric pairwise, K-pairwise, pairwise, and semiparametric independent models learned from the data for populations of various sizes. Specifically, we ask how often these models would make a mistake in predicting whether a chosen single neuron has fired or not. Every population response in our dataset corresponds to 20 ms of an experiment and so we can report this accuracy as number of errors per unit of time. Predictions based on the semiparametric pairwise model are consistently the most accurate.</p><p><xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5A</xref> shows the nonlinearities of the semiparametric pairwise models that we learned from data. In order to compare the nonlinearities inferred from populations of various sizes, we normalize the domain of the nonlinearity as well as its range by the number of neurons. Even though the nonlinearities could have turned out to have e.g. a sigmoidal shape, the general trend is that they are concave functions whose curvature&#8212;and thus departure from the linear <italic>V</italic> that signifies no global coupling&#8212;grows with the population size. The shape of these nonlinearities is reproducible over different subnetworks of the same size with very little variability. To further visualize the increasing curvature, we extrapolated what these nonlinearities might look like if the size of the population was very large (the black curve in <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5A</xref>). This extrapolation was done by subtracting an offset from each curve so that <italic>V</italic>(0) = 0, and then fitting a straight line to a plot of 1/<italic>N</italic> vs. the value of <italic>V</italic> at points uniformly spaced in the function&#8217;s domain. The plots of 1/<italic>N</italic> vs. <italic>V</italic> are only linear for <italic>N</italic> &#8805; 80, and so we only used these points for the extrapolation which is read out as the value of the fit when 1/<italic>N</italic> = 0. To quantify the increasing curvature, <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5B</xref> shows the average absolute value of the second derivative of <italic>V</italic> across the function&#8217;s domain.</p><fig id="pcbi.1005763.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g005</object-id><label>Fig 5</label><caption><title>Properties of the semiparametric pairwise model.</title><p><bold>A)</bold> Plot of <italic>V</italic> (<italic>E</italic>) vs <italic>E</italic>, i.e. the inferred nonlinearities of the semiparametric pairwise model. Curves are normalized by network size <italic>N</italic> and shifted along the y-axis for readability. Error bars (1 SD) denote variation over different subnetworks. The black curve is an extrapolation of the other curves to a large population size. <bold>B)</bold> The population size dependence of the average absolute value of the nonlinearity&#8217;s second derivative. Error bars (1 SD) denote variation over different subnetworks. <bold>C)</bold> Scatter plot of the couplings from a semiparametric pairwise model vs those from a pairwise model fitted to the whole population of 160 neurons. <bold>D)</bold> Comparison of the covariances predicted by the semiparametric pairwise model vs. those estimated from the training data. As an approximate guide for the sampling noise, covariances estimated from test data are also compared to covariances estimated from training data. Inset shows the same plot but with 10000 randomly sampled third moments <bold>E</bold>[<italic>s</italic><sub><italic>i</italic></sub><italic>s</italic><sub><italic>j</italic></sub><italic>s</italic><sub><italic>k</italic></sub>] such that <italic>i</italic> &#8800; <italic>j</italic> &#8800; <italic>k</italic> instead of the covariances.</p></caption><graphic xlink:href="pcbi.1005763.g005"/></fig><p>The coupling matrix <bold><italic>J</italic></bold> of both the pairwise and the semiparametric pairwise models describes effective interactions between neurons, and so it is interesting to ask how the couplings predicted by these two models are related. While <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5C</xref> shows a strong dependency between the couplings in a network of <italic>N</italic> = 160 neurons, the dependency is not deterministic and, moreover, negative couplings tend to be amplified in the semiparametric pairwise model as compared to the pairwise model. Similarly to the semiparametric independent model, there is no guarantee that the semiparametric pairwise model will reproduce observed pairwise correlations among neurons exactly, even though pairwise model has this guarantee by virtue of being a maximum entropy model. <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5D</xref> shows that despite the lack of such a guarantee, semiparametric pairwise model predicts a large majority of the correlations accurately, with the possible exceptions of several very strongly correlated pairs. This is simply because the semiparametric paiwise model is very accurate&#8211;the inset of <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5D</xref> shows that it can also reproduce third moments of the responses. A K-pairwise model also has this capability but, as shown in Ref [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>], a pairwise model systematically mispredicts higher than second moments.</p></sec><sec id="sec008"><title>Shape of the nonlinearity in critical models</title><p>Suppose we use the semiparametric pairwise model to analyze a very large population which is not globally coupled and can be divided into independent subpopulations. The only way the model in <xref ref-type="disp-formula" rid="pcbi.1005763.e016">Eq (9)</xref> can be factorized into a product of probability distributions over the subpopulations is if the function <italic>V</italic> is linear. Therefore, the prior knowledge that the population is not globally coupled immediately implies the shape of the nonlinearity. Similarly, a prior knowledge that the population is critical also carries a lot of information about the shape of the nonlinearity.</p><p>We show in Methods that if the parameters <bold><italic>&#945;</italic></bold> are known, then the optimal nonlinearity in <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> can be explicitly written as
<disp-formula id="pcbi.1005763.e019"><alternatives><graphic xlink:href="pcbi.1005763.e019.jpg" id="pcbi.1005763.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1005763.e020"><alternatives><graphic xlink:href="pcbi.1005763.e020.jpg" id="pcbi.1005763.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mrow><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the <italic>density of states</italic> which counts the number of patterns <bold>s</bold> whose energy is within some narrow range [<italic>E</italic>, <italic>E</italic> + &#916;]. The density of states is a central quantity in statistical physics that can be estimated also for neural activity patterns either directly from data or from inferred models [<xref rid="pcbi.1005763.ref019" ref-type="bibr">19</xref>]. Similarly, <inline-formula id="pcbi.1005763.e021"><alternatives><graphic xlink:href="pcbi.1005763.e021.jpg" id="pcbi.1005763.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mrow><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the empirical probability density of the energy <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) smoothed over the same scale &#916;. <xref ref-type="disp-formula" rid="pcbi.1005763.e019">Eq (12)</xref> follows from the relation <inline-formula id="pcbi.1005763.e022"><alternatives><graphic xlink:href="pcbi.1005763.e022.jpg" id="pcbi.1005763.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mrow><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8733;</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, i.e. the probability of some energy level is just the number of states with this energy times the probability of each of these states (see <xref ref-type="sec" rid="sec011">Methods</xref>).</p><p>We would like to establish a prior expectation on what the large <italic>N</italic> limit of the nonlinearites in <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5A</xref> is. Adapting the same normalization as in the figure, we denote <italic>&#1013;</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) = <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>)/<italic>N</italic>. Changing variables and rewriting <xref ref-type="disp-formula" rid="pcbi.1005763.e019">Eq (12)</xref> in terms of the empirical probability density of the normalized energy <inline-formula id="pcbi.1005763.e023"><alternatives><graphic xlink:href="pcbi.1005763.e023.jpg" id="pcbi.1005763.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mrow><mml:mover accent="true"><mml:mover accent="true"><mml:msub><mml:mi>p</mml:mi><mml:mi>&#1013;</mml:mi></mml:msub><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mi>N</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> yields
<disp-formula id="pcbi.1005763.e024"><alternatives><graphic xlink:href="pcbi.1005763.e024.jpg" id="pcbi.1005763.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mi>N</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mover accent="true"><mml:msub><mml:mi>p</mml:mi><mml:mi>&#1013;</mml:mi></mml:msub><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>N</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(13)</label></disp-formula>
For a system where <italic>s</italic><sub><italic>i</italic></sub> can take on two states, the total number of possible activity patterns is 2<sup><italic>N</italic></sup>, and so we expect the log of the density of states to be proportional to <italic>N</italic>. If the system is critical, then by virtue of <xref ref-type="disp-formula" rid="pcbi.1005763.e009">Eq (4)</xref>
<italic>&#963;</italic>(log <italic>p</italic><sub><italic>N</italic></sub>(<bold>s</bold>)) is proportional to N, and similarly we also expect <italic>&#963;</italic>(<italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>)) &#8733; <italic>N</italic>. This means that <italic>&#963;</italic>(<italic>&#1013;</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>)) = <italic>&#963;</italic>(<italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>))/<italic>N</italic> converges to some finite, nonzero number, and therefore <inline-formula id="pcbi.1005763.e025"><alternatives><graphic xlink:href="pcbi.1005763.e025.jpg" id="pcbi.1005763.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mover accent="true"><mml:msub><mml:mi>p</mml:mi><mml:mi>&#1013;</mml:mi></mml:msub><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> also stays finite no matter how large the population is. Taken together, for large critical populations, the first term on the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005763.e024">Eq (13)</xref> is the only one which scales linearly with the population size, and hence it dominates the other terms:
<disp-formula id="pcbi.1005763.e026"><alternatives><graphic xlink:href="pcbi.1005763.e026.jpg" id="pcbi.1005763.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8776;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(14)</label></disp-formula>
One of our important results is thus that for large critical populations, the nonlinearity should converge to the density of states of the inferred energy model. In other words, for critical systems as defined in <xref ref-type="disp-formula" rid="pcbi.1005763.e009">Eq (4)</xref>, there is a precise matching relation between the nonlinearity <italic>V</italic> (<italic>E</italic>) and the energy function <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>); in theory this is exact as <italic>N</italic> &#8594; &#8734;, but may hold approximately already at finite <italic>N</italic>.</p><p>To verify that this is the case for our neural population that has previously been reported to be critical, we compare in <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6A</xref> the nonlinearity inferred with the semiparametric pairwise model (<xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5A</xref>) to the density of states estimated using a Wang and Landau Monte Carlo algorithm [<xref rid="pcbi.1005763.ref040" ref-type="bibr">40</xref>] for a sequence of subpopulations of increasing size. As the population size increases, the nonlinearity indeed approaches the regime in which our prediction in <xref ref-type="disp-formula" rid="pcbi.1005763.e026">Eq (14)</xref> holds. This convergence is further quantified in <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6B</xref> which shows the average squared distance between the density of states and the nonlinearity. The average is taken over the range of observed energies. The nonlinearities are only specified up to an additive constant which we chose so as to minimize the squared distance between the density of states and the nonlinearity.</p><fig id="pcbi.1005763.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g006</object-id><label>Fig 6</label><caption><title>Properties of the inferred nonlinearity for neural networks of increasing size.</title><p><bold>A)</bold> Comparison between the inferred nonlinearity in the range of energies observed in the dataset and the log of the density of states at the same energies, showing the increasing match between the two quantities as the population size, <italic>N</italic>, increases. Both axes are normalized by the population size so that all curves have a similar scale. Nonlinearity can be shifted by an arbitrary constant without changing the model; to remove this redundancy, we set <italic>V</italic> (0) = 0 for all nonlinearities. <bold>B)</bold> The population size dependence of the average squared distance between the density of states and the inferred nonlinearity. Since the nonlinearity can be shifted by an arbitrary constant, we chose this offset so as to minimize the average squared distance. Error bars (1 SD) denote variation over different subnetworks. <bold>C)</bold> Inferred nonlinearities map to latent variables whose probability distributions can be computed and plotted for one sequence of subnetworks increasing in size (colors). As the network size increases, the dynamic range of the latent variable distribution does as well, which is quantified by the entropy of the distributions (inset).</p></caption><graphic xlink:href="pcbi.1005763.g006"/></fig></sec><sec id="sec009"><title>Mapping the nonlinearity to a latent variable</title><p>The link between global coupling and criticality is related to recent theoretical suggestions [<xref rid="pcbi.1005763.ref028" ref-type="bibr">28</xref>, <xref rid="pcbi.1005763.ref029" ref-type="bibr">29</xref>], where global coupling between the neurons in the population emerges as a result of shared latent (fluctuating) variables that simultaneously act on extensive subsets of neurons. In particular, Ref [<xref rid="pcbi.1005763.ref028" ref-type="bibr">28</xref>] theoretically analyzed models with a multivariate continuous latent variable <bold>h</bold> distributed according to some probability density <italic>q</italic>(<bold>h</bold>), whose influence on the population is described by the conditional probability distribution
<disp-formula id="pcbi.1005763.e027"><alternatives><graphic xlink:href="pcbi.1005763.e027.jpg" id="pcbi.1005763.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mspace width="1pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>O</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(15)</label></disp-formula>
where <italic>Z</italic><sub><italic>N</italic></sub>(<bold>h</bold>) is a normalization constant, and <inline-formula id="pcbi.1005763.e028"><alternatives><graphic xlink:href="pcbi.1005763.e028.jpg" id="pcbi.1005763.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are global quantities which sum over the whole population. The authors showed that under mild conditions on the probability density <italic>q</italic>(<bold>h</bold>) of <bold>h</bold>, and the scaling of <inline-formula id="pcbi.1005763.e029"><alternatives><graphic xlink:href="pcbi.1005763.e029.jpg" id="pcbi.1005763.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <italic>N</italic>, the sequence of models
<disp-formula id="pcbi.1005763.e030"><alternatives><graphic xlink:href="pcbi.1005763.e030.jpg" id="pcbi.1005763.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#8747;</mml:mo><mml:mspace width="1pt"/><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(16)</label></disp-formula>
is critical in the sense of <xref ref-type="disp-formula" rid="pcbi.1005763.e009">Eq (4)</xref>.</p><p>If the latent variable is one-dimensional, i.e. <bold>h</bold> = <italic>h</italic>, then the models in <xref ref-type="disp-formula" rid="pcbi.1005763.e030">Eq (16)</xref> have exactly the form of models in <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> with <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) = <italic>O</italic>(<bold>s</bold>), i.e. given a probability density <italic>q</italic>(<italic>h</italic>) of the latent variable, we can always find a nonlinearity <italic>V</italic> (<italic>E</italic>) such that
<disp-formula id="pcbi.1005763.e031"><alternatives><graphic xlink:href="pcbi.1005763.e031.jpg" id="pcbi.1005763.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>&#8734;</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>h</mml:mi><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:mi>h</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(17)</label></disp-formula>
The reverse problem of finding a latent variable for a given function <italic>V</italic> (<italic>E</italic>) such that this equation is satisfied does not always have a solution. The condition for this mapping to exist is that the function exp(&#8722;<italic>V</italic> (<italic>E</italic>)) is totally monotone [<xref rid="pcbi.1005763.ref041" ref-type="bibr">41</xref>], which, among other things, requires that it is convex. While our models allow for more general nonlinearites, we showed in <xref ref-type="fig" rid="pcbi.1005763.g005">Fig 5A</xref> that the inferred functions <italic>V</italic> (<italic>E</italic>) are concave and so we expect this mapping to be at least approximately possible (see below).</p><p>The mapping in <xref ref-type="disp-formula" rid="pcbi.1005763.e031">Eq (17)</xref> is based on a Laplace transformation, a technique commonly used for example in the study of differential equations. Laplace transformations are also often used in statistical physics where they relate the partition function of a system to its density of states. While the mathematics of Laplace transformations yields conditions on the function <italic>V</italic> (<italic>E</italic>) so that it is possible to map it to a latent variable (i.e., exp(&#8722;<italic>V</italic> (<italic>E</italic>)) must be totally monotone), analytically constructing this mapping is possible only in very special cases. We can gain a limited amount of intuition for this mapping by considering the case when the latent variable <italic>h</italic> is a narrow gaussian with mean <italic>h</italic><sub>0</sub> and variance <italic>&#963;</italic><sup>2</sup>. For small <italic>&#963;</italic><sup>2</sup>, one can show that
<disp-formula id="pcbi.1005763.e032"><alternatives><graphic xlink:href="pcbi.1005763.e032.jpg" id="pcbi.1005763.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8776;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(18)</label></disp-formula>
where <italic>E</italic><sub>0</sub> is the average energy if <italic>&#963;</italic><sup>2</sup> = 0, and the approximation holds only in a small neighborhood of <italic>E</italic><sub>0</sub> (|<italic>E</italic> &#8722; <italic>E</italic><sub>0</sub>| &#8810; <italic>&#963;</italic>). This approximation shows that the curvature of <italic>V</italic> (<italic>E</italic>) is proportional to the size of the fluctuations of the latent variable which, in turn, is expected to correlate with the amount of global coupling among neurons.</p><p>This relationship to global coupling can be understood from the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005763.e031">Eq (17)</xref>. When the energy function is, for example, a weighted sum of individual neurons as in the semiparametric independent model of <xref ref-type="disp-formula" rid="pcbi.1005763.e015">Eq (8)</xref>, then we can think of <xref ref-type="disp-formula" rid="pcbi.1005763.e031">Eq (17)</xref> as a latent variable <italic>h</italic> (perhaps reflecting the stimulus) coupled to every neuron, and hence inducing a coupling between the whole population. A non-neuroscience example is that of a scene with <bold>s</bold> representing the luminance in each pixel, and the latent <italic>h</italic> representing the lighting conditions which influence all the pixels simultaneously.</p><p>We used the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005763.e031">Eq (17)</xref> (see <xref ref-type="sec" rid="sec011">Methods</xref>) to infer the shapes of the probability densities of the latent variables which correspond to the nonlinearities in the semiparametric pairwise models learned from data. These probability densities are shown in <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6C</xref>. A notable difference to the formulation in <xref ref-type="disp-formula" rid="pcbi.1005763.e030">Eq (16)</xref> is that the inferred latent variables scale with the population size; in particular, the inset to <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6C</xref> shows that the entropy of the inferred latent variable increases with the population size. Entropy is a more appropriate measure of the &#8220;broadness&#8221; of a probability density than standard deviation when the density is multimodal. Taken together with the results in <xref ref-type="fig" rid="pcbi.1005763.g004">Fig 4A</xref>, this suggests that global coupling is especially important for larger populations. However, it is also possible that the latents are becoming broader because the model is trying to compensate for limited capacity, and that the entropy of the latent would saturate if we had a more expressive energy function. Larger datasets and/or further improvements in probabilistic models are necessary to make more detailed conclusions.</p><p>Interestingly, the probability densities of the latent variables consist of two modes at approximately <italic>h</italic> = 0.7 and <italic>h</italic> = 1.3. We hypothesize that these modes reflect a discrete-like nature of the population dynamics which consist of bursts of activity interspaced with periods of approximate silence. These bursts are demonstrated in <xref ref-type="fig" rid="pcbi.1005763.g007">Fig 7A</xref> where we show the time dependence of the total network activity. Unfortunately, closer inspection reveals that the total network activity cannot be used in a straightforward manner to classify the population as active or inactive. The reason is that neurons are noisy and if we defined a population as inactive when the total network activity is 0, then such definition is not robust to noise. In fact, the probability distribution of the total network activity (<xref ref-type="fig" rid="pcbi.1005763.g003">Fig 3A</xref>) is such that there is no obvious choice of a threshold, and so quantifying the discreteness of the population dynamics based on the total network activity would be arbitrary.</p><fig id="pcbi.1005763.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1005763.g007</object-id><label>Fig 7</label><caption><title>The most likely value of the latent variable naturally defines two global population states.</title><p><bold>A)</bold> For every repeat of the stimulus and for every time bin we estimate the most likely value of the latent (<italic>h</italic>*) given the population response at that time, as well as the total number of spiking neurons in that response (<italic>K</italic>). The plot shows the trajectories of <italic>h</italic>* and <italic>K</italic> averaged across repeats. Error bars correspond to 1 SD. <bold>B)</bold> Probability density of <italic>h</italic>*, i.e. the most likely value of the latent given the population response. <bold>C)</bold> A scatter plot of the total network activity vs. the most likely value of the latent. <italic>h</italic>* naturally divides the population responses into two clusters. <bold>D)</bold> Probability distribution of the total network activity given this global population state. While the most likely value of <italic>K</italic> for low <italic>h</italic>* is zero, the distribution has a tail that extends to <italic>K</italic> &#8776; 5.</p></caption><graphic xlink:href="pcbi.1005763.g007"/></fig><p>To circumvent these problems and enable a robust classification of the population state as active or inactive, we can use the most likely value of the latent variable given a population response, i.e.
<disp-formula id="pcbi.1005763.e033"><alternatives><graphic xlink:href="pcbi.1005763.e033.jpg" id="pcbi.1005763.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">argmax</mml:mo><mml:mi>h</mml:mi></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">argmax</mml:mo><mml:mi>h</mml:mi></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">argmax</mml:mo><mml:mi>h</mml:mi></mml:munder><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>h</mml:mi><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(19)</label></disp-formula>
<xref ref-type="fig" rid="pcbi.1005763.g007">Fig 7A</xref> shows the time dependence of <italic>h</italic>*, and <xref ref-type="fig" rid="pcbi.1005763.g007">Fig 7B</xref> its probability density (estimated by collecting <italic>h</italic>*(<bold>s</bold>) over all repeats and times). The probability density of <italic>h</italic>* has two modes separated by an inaccessible region, so one can easily classify a population response <bold>s</bold> as active or inactive based on which mode <italic>h</italic>*(<bold>s</bold>) belongs to. <xref ref-type="fig" rid="pcbi.1005763.g007">Fig 7C and 7D</xref> show that a population pattern with, for example, 5 active neurons can have very different values for <italic>h</italic>*(<bold>s</bold>), demonstrating that any measure based on the total network activity would easily confuse which state the population is in.</p></sec></sec><sec sec-type="conclusions" id="sec010"><title>Discussion</title><p>Criticality is a theoretical concept which depends crucially on how the probability distribution over population activity patterns scales with the population size. Constructing this scaling directly from data is complicated, and necessarily involves extrapolating to large population sizes [<xref rid="pcbi.1005763.ref010" ref-type="bibr">10</xref>, <xref rid="pcbi.1005763.ref030" ref-type="bibr">30</xref>]. As a consequence, answering the question whether a population is critical or &#8220;how close to critical&#8221; it is, is difficult. Here we took a different approach&#8212;we used the theoretical notion of criticality to guide our intuition about what models are useful for analyzing populations that exhibit signs of criticality such as an approximate Zipf law. From the standpoint of fitting statistical models, it is irrelevant whether or not the studied population is really critical given some operational realization of the large population size limit because our models can be used either way, and their accuracy can be evaluated using standard model selection techniques. In particular, our approach is agnostic to the origins of the signatures of criticality which have been hotly debated [<xref rid="pcbi.1005763.ref025" ref-type="bibr">25</xref>&#8211;<xref rid="pcbi.1005763.ref030" ref-type="bibr">30</xref>, <xref rid="pcbi.1005763.ref042" ref-type="bibr">42</xref>]. Our reasoning is thus very pragmatic: we on purpose avoided the controversial (albeit interesting) issues of whether the observed critical behavior in real data is &#8220;trivial&#8221; or not and what may be its mechanistic explanation, and focused rather on making use of the observation itself to design better probabilistic models for neural code.</p><p>This pragmatic approach is driven by the rapid development of experimental techniques for recording the activity of large neural populations, which is posing a challenge for data analysis. The number of neurons that we can measure simultaneously is growing much faster than the time period over which we can record from these neurons. Therefore, we might soon be in a regime where the number of available samples is comparable to the population size. To make meaningful conclusions from such datasets, our models will need to take maximal advantage of the prior knowledge about the dependency structure among neurons. The prior knowledge that the distribution of activity could be close to critical and that the population could be globally coupled are two macroscopic features of the neural code that future models should be able to reproduce without extreme tuning of many parameters. Our semiparametric energy models directly utilize this prior knowledge, and because the complexity of the nonlinearity is held fixed for all population sizes, it can be easily used in models with arbitrary number of neurons.</p><p>While today&#8217;s neuroscience provides us with sufficient data to build accurate models of neural populations, it is also important that these models generate new hypotheses and shape the direction of future research. For example, our goal was not to trace the origins of the observed Zipf law, but we nevertheless believe that the pursuit of these origins can only happen in a data-driven context to which our models will further contribute. There are many toy models that reproduce Zipf law, several of which have been proposed in the neuroscience context to additionally account for related signatures of criticality, e.g., the behavior of the heat capacity. Some of these models invoked the particular structure of the observed pairwise correlations, ascribed specific importance to fluctuating (latent) variables (see Discussion in [<xref rid="pcbi.1005763.ref019" ref-type="bibr">19</xref>]) which could (or not) be directly related to the stimulus itself, or suggested that the processes of model construction, inference, or scaling to large <italic>N</italic> generate spurious signatures of criticality. The issue is thus not the lack of possible explanations. Rather, it is that these explanations account qualitatively for only one selected aspect of the actual data, while not truly testing whether the proposed explanation is quantitatively consistent with <italic>all</italic> of the reported phenomena and measured statistics. Here, we took seriously the idea that the signatures of criticality could be due to a global coupling to a hidden (latent) fluctuating variable, as proposed and discussed in the context of a blowfly motion-sensitive neuron in Ref [<xref rid="pcbi.1005763.ref028" ref-type="bibr">28</xref>], and we have shown that the proposed mechanism is viable in a model that precisely accounts for a real and well-studied dataset [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>].</p><p>It is important to stress that the identified latent variable is only an effective description of the data, and so, without further experiments, we cannot interpret it in terms of some biophysical mechanisms, nor can we claim, for example, that the population is critical because of this latent variable. However, knowing that this latent variable is a useful statistic describing the population should be a motivation for designing future experiments so that we can correlate it with more detailed mechanisms on the level of neural circuits, and possibly gain insight into its bimodal structure. It also suggests that we should analyze populations responding to various stimuli so that we can understand the latent variable&#8217;s stimulus dependence. The scaling of the latent variable shown in <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6C</xref> also suggests that we should reexamine whether we could find even better description of the data with more than one latent variables. This could be done by studying models with multiple or with multidimensional nonlinearities. Generally, these models have the form log <italic>p</italic>(<bold>s</bold>) &#8733; <italic>V</italic> (<italic>E</italic><sub>1</sub>(<bold>s</bold>), <italic>E</italic><sub>2</sub>(<bold>s</bold>), &#8230;), and a particularly interesting special case is when each &#8220;energy&#8221; function <italic>E</italic><sub><italic>i</italic></sub> is a simple linear projection of the responses as in the semiparametric independent model. These models offer an avenue for both improving the accuracy and reducing the number of parameters. In light of the theoretical analysis in Ref [<xref rid="pcbi.1005763.ref028" ref-type="bibr">28</xref>], each dimension of the nonlinearity could possibly be interpreted as a separate latent variable. While we are not aware of general conditions which would guarantee that a multidimensional nonlinearity can be mapped to a multidimensional latent variable, intuition suggests that as the dimension of the nonlinearity increases, the space of nonlinearities which allow for this inversion becomes smaller. This means that if we fit a model with a general multidimensional nonlinearity to data, and we find that this nonlinearity can be mapped to a multidimensional latent variable, then it is an evidence that these latent variables can be correlated with actual physical mechanisms which can be sought for in future experiments.</p><p>There exist alternative ways of modeling global coupling (and thus likely capturing signatures of criticality) in neural populations. Hidden-Markov-Model-type (HMM) models have been considered for the retinal data [<xref rid="pcbi.1005763.ref043" ref-type="bibr">43</xref>], where the discrete hidden states correspond to collective modes of activity that, due to noise in neural spiking, map probabilistically into observed activity patterns of spiking and silence. In contrast, our model can be interpreted as having a single (but continuous) hidden variable&#8212;although we empirically find that the distribution of this latent variable is actually bimodal, highlighting the basic distinction between the &#8220;silent&#8221; or &#8220;inactive&#8221; state of the retina, and the &#8220;active&#8221; state [<xref rid="pcbi.1005763.ref044" ref-type="bibr">44</xref>]. The HMM models were introduced to capture more flexibly collective modes of activity first observed in pairwise and K-pairwise models [<xref rid="pcbi.1005763.ref010" ref-type="bibr">10</xref>, <xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>]. Unlike the semiparametric pairwise model, they take into account the observed temporal dynamics, and they are also parametrically richer. Furthermore, their learned hidden states show interesting correspondence to the displayed stimuli even though the model is a priori agnostic about the stimulus. On the other hand, the HMM models admit no clear link to and interpretations of the signatures of criticality, which was our motivation in this paper. Related to the HMMs, [<xref rid="pcbi.1005763.ref045" ref-type="bibr">45</xref>, <xref rid="pcbi.1005763.ref046" ref-type="bibr">46</xref>] discuss another classes of accurate models which capture the temporal dynamics of the population.</p><p>Unlike HMMs and related models, this paper is concerned with modeling the stationary distribution rather than the precise time-dependence of the population. While this discards a lot of information, and hence the resulting models are possibly less accurate, there are advantages to focusing on stationary models. On the technical side, temporal models require more parameters and associated decisions about how to represent the stimulus and its interactions with the population, and so they are harder to scale to datasets with large numbers of neurons. More importantly, however, it was precisely by disregarding the temporal information that the ubiquity of criticality and the role of weak pairwise correlations [<xref rid="pcbi.1005763.ref008" ref-type="bibr">8</xref>] in neural populations were discovered. It is thus possible that discarding the temporal information allows us to make more general observations about neural codes. This is an important hypothesis. For example, the models we consider in this work, as well as most of published models, are accurate only when applied to data collected in a very narrow experimental context, and it is unclear if/how much would these models generalize to novel stimuli/experimental conditions, nor is it obvious how to design experiments so that we can infer models which generalize as much as possible. While it remains to be tested, it is an intriguing hypothesis that stationary models have more potential for generalization across experiments.</p><p>In the domain of stationary models, Restricted Boltzmann Machines (RBMs) and their derivatives [<xref rid="pcbi.1005763.ref034" ref-type="bibr">34</xref>] are also classes of energy-based models for population activity that could capture global coupling by latent variables. RBMs are universal learners that, given sufficient data, can reproduce any distribution&#8212;including a critical one; like HMM models, however, making a generic link between their parameters and criticality appears difficult. We note that the RBM structure is not incompatible with the structure of semiparametric energy-based models: one could consider a &#8220;semiparametric RBM model,&#8221; where <italic>E</italic> in <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq 3</xref> is defined by a RBM, whose parameters are learned jointly with the nonlinearity, <italic>V</italic> (<italic>E</italic>).</p><p>A different class of models that has been demonstrated to capture criticality consists of various derivatives of the dichotomized Gaussian model [<xref rid="pcbi.1005763.ref026" ref-type="bibr">26</xref>, <xref rid="pcbi.1005763.ref039" ref-type="bibr">39</xref>, <xref rid="pcbi.1005763.ref047" ref-type="bibr">47</xref>]. A comparison between the dichotomized Gaussian, pairwise, and K-pairwise models on the same dataset as we consider in this work was done in [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>]. They showed that while the dichotomized Gaussian is comparable to the pairwise model, the K-pairwise model, and hence also the semiparametric pairwise model, are more accurate. The analysis in [<xref rid="pcbi.1005763.ref048" ref-type="bibr">48</xref>, <xref rid="pcbi.1005763.ref049" ref-type="bibr">49</xref>] shows that the distribution of the total network activity (as in <xref ref-type="fig" rid="pcbi.1005763.g003">Fig 3</xref>) can often be fitted using a generalization of the dichotomized Gaussian model in which the inputs are q-Gaussians, but they assume that all neurons are the same and do not aim to model more detailed statistics of the neural responses. More recently [<xref rid="pcbi.1005763.ref050" ref-type="bibr">50</xref>] discusses how to extend the dichotomized q-Gaussian model to heterogeneous populations. However, they only show how to use this model to match the observed means and pairwise correlations while keeping the <italic>q</italic> parameter fixed, and they do not discuss how to perform maximum likelihood inference on all parameters simultaneously. Since these studies on the dichotomized q-Gaussian model showed that the <italic>q</italic> parameter is relevant for statistics related to global coupling, it would be an interesting research direction to develop a procedure for maximum likelihood inference of this model, and compare it to the semiparametric pairwise model.</p><p>The observations of criticality in real data are not specific to neuroscience. Datasets in many other fields such as luminance in natural images [<xref rid="pcbi.1005763.ref031" ref-type="bibr">31</xref>], or amino acid sequences of proteins [<xref rid="pcbi.1005763.ref033" ref-type="bibr">33</xref>] have been shown to exhibit Zipf law. In particular, models of the form Eqs (<xref ref-type="disp-formula" rid="pcbi.1005763.e007">3</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005763.e031">17</xref>) have been used to model the statistics of small image patches under the name elliptically symmetric distributions and Gaussian scale mixtures [<xref rid="pcbi.1005763.ref051" ref-type="bibr">51</xref>, <xref rid="pcbi.1005763.ref052" ref-type="bibr">52</xref>] although the motivation for using these models had nothing to do with criticality. These models are much easier to analyze than the models we consider in this paper because the variables <italic>s</italic><sub><italic>i</italic></sub> are continuous rather than discrete. Our discussion regarding <xref ref-type="disp-formula" rid="pcbi.1005763.e026">Eq (14)</xref> and the prior expectations about the shape of the nonlinearity is valid even in the continuous case. In particular, elliptically symmetric distributions are essentially the same as our semiparametric pairwise models, <xref ref-type="disp-formula" rid="pcbi.1005763.e016">Eq (9)</xref>, only with continuous variables. Because <italic>s</italic><sub><italic>i</italic></sub> are continuous, we can analytically evaluate the density of states,
<disp-formula id="pcbi.1005763.e034"><alternatives><graphic xlink:href="pcbi.1005763.e034.jpg" id="pcbi.1005763.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8733;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(20)</label></disp-formula>
and so the optimal nonlinearity for an elliptically symmetric distribution fitted to a large system which exhibits criticality (e.g. image patches) is expected to be <italic>V</italic> (<italic>E</italic>) = (<italic>N</italic>/2 &#8722; 1) log <italic>E</italic> + const.</p><p>Another connection between our models and a substantial body of theoretical work is in the context of nonextensive statistical mechanics. Physicists have considered models of the form Eqs (<xref ref-type="disp-formula" rid="pcbi.1005763.e007">3</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005763.e031">17</xref>) as models of systems whose entropy grows sublinearly with the system size [<xref rid="pcbi.1005763.ref053" ref-type="bibr">53</xref>]. It is difficult to make these connections explicit because nonextensive statistical mechanics has been studied mostly through toy models rather than data-driven generative models that we examine here; furthermore, in the toy models the latent variables are usually assumed to converge to a delta function as the population size grows which is in stark contrast with our findings in <xref ref-type="fig" rid="pcbi.1005763.g006">Fig 6</xref>. Nevertheless, deepening the connection between models inferred from data, the maximum entropy formalism itself (e.g., considering the possibility that our semiparametric energy models of <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> can be derived from the maximization of a generalized version of the standard entropy), and nonextensive statistical mechanics is an interesting topic for further research.</p></sec><sec sec-type="materials|methods" id="sec011"><title>Methods</title><sec id="sec012"><title>Relation of the nonlinearity to the probability density of the energy</title><p>Let <italic>&#961;</italic>(<italic>E</italic>&#8242;; <bold><italic>&#945;</italic></bold>) = &#8721;<sub><italic>s</italic></sub>
<italic>&#948;</italic><sub><italic>E</italic>&#8242;, <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>)</sub> count the number of states which map to the same energy <italic>E</italic>&#8242;. The probability distribution of <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) when <bold>s</bold> is distributed according to <xref ref-type="disp-formula" rid="pcbi.1005763.e007">Eq (3)</xref> is
<disp-formula id="pcbi.1005763.e035"><alternatives><graphic xlink:href="pcbi.1005763.e035.jpg" id="pcbi.1005763.e035g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="bold">s</mml:mi></mml:munder><mml:mspace width="1pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(21)</label></disp-formula>
Given data <inline-formula id="pcbi.1005763.e036"><alternatives><graphic xlink:href="pcbi.1005763.e036.jpg" id="pcbi.1005763.e036g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M36"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, let <inline-formula id="pcbi.1005763.e037"><alternatives><graphic xlink:href="pcbi.1005763.e037.jpg" id="pcbi.1005763.e037g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M37"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mspace width="1pt"/><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> be the data distribution of the energy, and let &#937;<sub><italic>&#945;</italic></sub> be the image of <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>). The average log-likelihood of the data can be rewritten as
<disp-formula id="pcbi.1005763.e038"><alternatives><graphic xlink:href="pcbi.1005763.e038.jpg" id="pcbi.1005763.e038g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M38"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msub><mml:mo>&#937;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mspace width="1pt"/><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msub><mml:mo>&#937;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mspace width="1pt"/><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msub><mml:mo>&#937;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mspace width="1pt"/><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(22)</label></disp-formula>
where the third line follows by substituting the logarithm of <xref ref-type="disp-formula" rid="pcbi.1005763.e035">Eq (21)</xref>.</p><p><xref ref-type="disp-formula" rid="pcbi.1005763.e038">Eq (22)</xref> has a simple interpretation. The last term, which is the only one depending on <italic>V</italic>, is the average log-likelihood of the samples <inline-formula id="pcbi.1005763.e039"><alternatives><graphic xlink:href="pcbi.1005763.e039.jpg" id="pcbi.1005763.e039g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M39"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> under the model <italic>p</italic>(<italic>E</italic>; <bold><italic>&#945;</italic></bold>, <italic>V</italic>), and so, for any <bold><italic>&#945;</italic></bold>, the purpose of the nonlinearity is to reproduce the data probability distribution of the energy.</p><p>Our restriction that <italic>V</italic> is a twice differentiable increasing function can be seen as a way of regularizing learning. The last term in <xref ref-type="disp-formula" rid="pcbi.1005763.e038">Eq (22)</xref> is the negative cross entropy between <inline-formula id="pcbi.1005763.e040"><alternatives><graphic xlink:href="pcbi.1005763.e040.jpg" id="pcbi.1005763.e040g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M40"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <italic>p</italic>(<italic>E</italic>; <bold><italic>&#945;</italic></bold>, <italic>V</italic>) and it is well known that this term is maximal if <inline-formula id="pcbi.1005763.e041"><alternatives><graphic xlink:href="pcbi.1005763.e041.jpg" id="pcbi.1005763.e041g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M41"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. According to <xref ref-type="disp-formula" rid="pcbi.1005763.e035">Eq (21)</xref>, if <italic>V</italic> was arbitrary, then, for any <bold><italic>&#945;</italic></bold>, we can satisfy this equality with any (possibly infinite) function <italic>V</italic> such that
<disp-formula id="pcbi.1005763.e042"><alternatives><graphic xlink:href="pcbi.1005763.e042.jpg" id="pcbi.1005763.e042g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>const.</mml:mtext><mml:mspace width="2.em"/><mml:mtext>for</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>all</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mo>&#937;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(23)</label></disp-formula>
If the energy function assigns distinct energies to distinct states, then the choice in <xref ref-type="disp-formula" rid="pcbi.1005763.e042">Eq (23)</xref> leads to a model which exactly reproduces the empirical distribution of data, and hence overfits.</p><p>An alternative way of regularizing would be to assume that <italic>V</italic> is a piecewise constant function. In that case, the analog of <xref ref-type="disp-formula" rid="pcbi.1005763.e042">Eq (23)</xref> is
<disp-formula id="pcbi.1005763.e043"><alternatives><graphic xlink:href="pcbi.1005763.e043.jpg" id="pcbi.1005763.e043g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>const.</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(24)</label></disp-formula>
where, for every bin on which <italic>V</italic> is constant, the density of states <inline-formula id="pcbi.1005763.e044"><alternatives><graphic xlink:href="pcbi.1005763.e044.jpg" id="pcbi.1005763.e044g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M44"><mml:mrow><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> counts the number of states whose energy maps to this bin divided by the bin width. Similarly, the empirical energy density <inline-formula id="pcbi.1005763.e045"><alternatives><graphic xlink:href="pcbi.1005763.e045.jpg" id="pcbi.1005763.e045g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M45"><mml:mrow><mml:mover accent="true"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> counts the number of samples whose energy maps to this bin divided by the bin width.</p></sec><sec id="sec013"><title>Learning the models</title><p>All models were trained using a variation of Persistent Contrastive Divergence [<xref rid="pcbi.1005763.ref038" ref-type="bibr">38</xref>] which performs an approximate gradient ascent on the log-likelihood for any model of the form <italic>p</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) = exp(&#8722;<italic>F</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>))/<italic>Z</italic>(<bold><italic>&#945;</italic></bold>), where <italic>F</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) is a computationally tractable function differentiable in the parameters <bold><italic>&#945;</italic></bold>, and <italic>Z</italic>(<bold><italic>&#945;</italic></bold>) is a normalization constant. Given an initial guess of the parameters <bold><italic>&#945;</italic></bold><sub>0</sub>, and a list of <italic>M</italic><sub><italic>s</italic></sub> samples drawn from <italic>p</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold><sub>0</sub>), the algorithm can be summarized as</p><p specific-use="line">for <italic>t</italic> &#8788; 1 to <italic>L</italic></p><p specific-use="line">&#8195; <bold><italic>&#945;</italic></bold><sub><italic>t</italic></sub> = <bold><italic>&#945;</italic></bold><sub><italic>t</italic>&#8722;1</sub> + <italic>&#951;</italic>(<bold>E</bold>[&#8711;<sub><bold><italic>&#945;</italic></bold></sub>
<italic>F</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold><sub><italic>t</italic>&#8722;1</sub>)]<sub>samples<sub><italic>t</italic>&#8722;1</sub></sub> &#8722; <bold>E</bold>[&#8711;<sub><bold><italic>&#945;</italic></bold></sub>
<italic>F</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold><sub><italic>t</italic>&#8722;1</sub>)]<sub>data</sub>)</p><p specific-use="line">&#8195; samples<sub><italic>t</italic></sub> = GIBBS<sup><italic>n</italic></sup> (samples<sub><italic>t</italic>&#8722;1</sub>, <bold>&#945;</bold><sub><italic>t</italic></sub>)</p><p>where <italic>L</italic> is the number of iterations, <italic>&#951;</italic> is the learning rate, <bold>E</bold>[&#8901;]<sub>list</sub> denotes an average over the list of states, and GIBBS<sup><italic>n</italic></sup> represents <italic>n</italic> applications of the Gibbs sampling transition operator.</p><p>Pairwise and K-pairwise models were trained using <italic>&#951;</italic> = 1, <italic>n</italic> = 2<italic>N</italic>, and with initial parameters drawn from a normal distribution with 0 mean and 0.1 standard deviation. We iterated the algorithm two times, first with <italic>L</italic> = 10000, <italic>M</italic><sub><italic>s</italic></sub> = 3 &#215; 10<sup>4</sup>, then with <italic>L</italic> = 10000, <italic>M</italic><sub><italic>s</italic></sub> = 3 &#215; 10<sup>5</sup>. Semiparametric independent and pairwise models were trained using <italic>&#951;</italic> = 5 &#215; 10<sup>&#8722;5</sup> for the parameters of the function <italic>V</italic> (see below), and <italic>&#951;</italic> = 1 for all other parameters. We initialized the model with parameters corresponding to the learned independent (pairwise) models, and trained for <italic>L</italic> = 10000 iterations with <italic>M</italic><sub><italic>s</italic></sub> = 3 &#215; 10<sup>4</sup> samples.</p><p>The function V is parametrized through a function W (see <xref ref-type="disp-formula" rid="pcbi.1005763.e014">Eq (7)</xref>). We use piecewise constant functions to parametrize <italic>W</italic>. Let [<italic>E</italic><sub>0</sub>, <italic>E</italic><sub>1</sub>] be an interval containing the range of energies <italic>E</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) which we expect to encounter during learning. We divide the interval [<italic>E</italic><sub>0</sub>, <italic>E</italic><sub>1</sub>] into <italic>Q</italic> non-overlapping bins of the same width with indicator functions <italic>I</italic><sub><italic>i</italic></sub>, i.e. <italic>I</italic><sub><italic>i</italic></sub>(<italic>E</italic>) = 1 if <italic>E</italic> is in the <italic>i</italic>th bin, otherwise <italic>I</italic><sub><italic>i</italic></sub>(<italic>E</italic>) = 0, and we set <inline-formula id="pcbi.1005763.e046"><alternatives><graphic xlink:href="pcbi.1005763.e046.jpg" id="pcbi.1005763.e046g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M46"><mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8801;</mml:mo><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>Q</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. We used <italic>Q</italic> = 20 bins in all experiments. This was a conservative choice: increasing <italic>Q</italic> did not result in a higher training or validation likelihood.</p><p>The integrals in <xref ref-type="disp-formula" rid="pcbi.1005763.e014">Eq (7)</xref> can be carried out analytically for this choice of <italic>W</italic> yielding an exact expression for <italic>V</italic> as a function of <bold><italic>&#947;</italic></bold> and <bold><italic>&#946;</italic></bold>. For <italic>E</italic> &lt; <italic>E</italic><sub>0</sub>, we have <italic>V</italic> (<italic>E</italic>; <italic>&#947;</italic>, <bold><italic>&#946;</italic></bold>) = <italic>&#947;</italic><sub>1</sub> + <italic>&#947;</italic><sub>2</sub>(<italic>E</italic> &#8722; <italic>E</italic><sub>0</sub>). For <italic>E</italic> &gt; <italic>E</italic><sub>0</sub> we have <italic>V</italic> (<italic>E</italic>; <italic>&#947;</italic>, <bold><italic>&#946;</italic></bold>) = <italic>&#947;</italic><sub>1</sub> + <italic>&#947;</italic><sub>2</sub>
<italic>f</italic>(<italic>E</italic>; <bold><italic>&#946;</italic></bold>), where
<disp-formula id="pcbi.1005763.e047"><alternatives><graphic xlink:href="pcbi.1005763.e047.jpg" id="pcbi.1005763.e047g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mi>E</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msubsup><mml:mspace width="1pt"/><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#916;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(25)</label></disp-formula>
We define [<italic>E</italic>] as the number of the bin that contains <italic>E</italic>. If <italic>E</italic> &gt; <italic>E</italic><sub>1</sub>, then we define [<italic>E</italic>] = <italic>Q</italic> + 1, and <italic>&#946;</italic><sub><italic>Q</italic>+1</sub> = 0.</p><p>Using this expression we can calculate the gradients &#8711;<sub><bold><italic>&#945;</italic></bold></sub>
<italic>F</italic>(<bold>s</bold>; <bold><italic>&#945;</italic></bold>) in the algorithm exactly. This calculation is straightforward although the resulting expressions are cumbersome. For the semiparametric pairwise model, we have <inline-formula id="pcbi.1005763.e048"><alternatives><graphic xlink:href="pcbi.1005763.e048.jpg" id="pcbi.1005763.e048g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M48"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The gradient with respect to the couplings is
<disp-formula id="pcbi.1005763.e049"><alternatives><graphic xlink:href="pcbi.1005763.e049.jpg" id="pcbi.1005763.e049g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M49"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(26)</label></disp-formula>
The gradients with respect to <bold><italic>&#947;</italic></bold> and <bold><italic>&#946;</italic></bold> are just the gradients of <italic>V</italic> (<italic>E</italic>; <bold><italic>&#947;</italic></bold>, <bold><italic>&#946;</italic></bold>) with respect to these parameters and they are as follows:
<disp-formula id="pcbi.1005763.e050"><alternatives><graphic xlink:href="pcbi.1005763.e050.jpg" id="pcbi.1005763.e050g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>V</mml:mi><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(27)</label></disp-formula>
<disp-formula id="pcbi.1005763.e051"><alternatives><graphic xlink:href="pcbi.1005763.e051.jpg" id="pcbi.1005763.e051g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(28)</label></disp-formula>
<disp-formula id="pcbi.1005763.e052"><alternatives><graphic xlink:href="pcbi.1005763.e052.jpg" id="pcbi.1005763.e052g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M52"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(29)</label></disp-formula>
If <italic>k</italic> &gt; [<italic>E</italic>], then
<disp-formula id="pcbi.1005763.e053"><alternatives><graphic xlink:href="pcbi.1005763.e053.jpg" id="pcbi.1005763.e053g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(30)</label></disp-formula>
If <italic>k</italic> = [<italic>E</italic>], then
<disp-formula id="pcbi.1005763.e054"><alternatives><graphic xlink:href="pcbi.1005763.e054.jpg" id="pcbi.1005763.e054g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(31)</label></disp-formula>
If <italic>k</italic> &lt; [<italic>E</italic>], then
<disp-formula id="pcbi.1005763.e055"><alternatives><graphic xlink:href="pcbi.1005763.e055.jpg" id="pcbi.1005763.e055g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>&#946;</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mspace width="1pt"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mspace width="1pt"/><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#916;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(32)</label></disp-formula></p></sec><sec id="sec014"><title>Estimating likelihoods</title><p>Data likelihoods cannot be evaluated exactly because the normalization constants <italic>Z</italic> are intractable. We resorted to Monte Carlo method known as thermodynamic integration in physics [<xref rid="pcbi.1005763.ref054" ref-type="bibr">54</xref>], and annealed importance sampling in machine learning, to estimate the normalization constants [<xref rid="pcbi.1005763.ref055" ref-type="bibr">55</xref>]. The initial model for annealed importance sampling was always the independent model for which the partition function can be calculated exactly. The sampling procedure consisted of 10<sup>4</sup> intermediate distributions which uniformly interpolated from the independent model to the model of interest. Each partition function was estimated using 10<sup>4</sup> samples.</p><p>All reported likelihoods were evaluated on held-out data. A simple cross-validation also showed that our models did not suffer from overfitting.</p></sec><sec id="sec015"><title>Estimating the density of states and the latent variables</title><p>Density of states was estimated using the Wang and Landau algorithm [<xref rid="pcbi.1005763.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1005763.ref040" ref-type="bibr">40</xref>]. The accuracy parameter (the smallest increment size for the log of the density of states) was 10<sup>&#8722;7</sup>. The energy range was estimated during the first few thousand steps of the algorithm. This range was divided into &#8764; 10<sup>4</sup> bins. We decreased the increment size every &#8764; 10<sup>8</sup> iterations instead of checking energy histogram flatness since the later is hard to do when some energy bins are inaccessible.</p><p>We inferred the probability densities of the latent variables by considering the model in <xref ref-type="disp-formula" rid="pcbi.1005763.e031">Eq (17)</xref> with fixed <bold><italic>J</italic></bold> which corresponds to the coupling matrix of the previously learned semiparametric pairwise model. The domain of the latent variable was set to [0, 5]. We approximated the integral with a sum by dividing this domain into 400 bins, and the value of the probability density <italic>q</italic>(<italic>h</italic>) was inferred by maximizing the likelihood of data subject to the constraint that <italic>q</italic>(<italic>h</italic>) integrates to 1. To make the computation tractable, we needed an expression for <italic>Z</italic>(<italic>h</italic>; <bold><italic>J</italic></bold>). This can be obtained from the estimated density of states <italic>&#961;</italic>(<italic>E</italic>; <bold><italic>J</italic></bold>) of the energy as
<disp-formula id="pcbi.1005763.e056"><alternatives><graphic xlink:href="pcbi.1005763.e056.jpg" id="pcbi.1005763.e056g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M56"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="bold">s</mml:mi></mml:munder><mml:mspace width="1pt"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>h</mml:mi><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>&#8747;</mml:mo><mml:mspace width="1pt"/><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>h</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mspace width="1pt"/><mml:mi mathvariant="normal">d</mml:mi><mml:mi>E</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(33)</label></disp-formula></p></sec></sec></body><back><ack><p>We thank David Schwab, Elad Schneidman, and William Bialek for helpful discussions, as well as Ilya Nemenman for valuable feedback.</p></ack><ref-list><title>References</title><ref id="pcbi.1005763.ref001"><label>1</label><mixed-citation publication-type="book">
<name><surname>Rieke</surname><given-names>F</given-names></name>, <name><surname>Warland</surname><given-names>D</given-names></name>, <name><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <source>Spikes: Exploring the Neural Code</source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1999</year>.</mixed-citation></ref><ref id="pcbi.1005763.ref002"><label>2</label><mixed-citation publication-type="book">
<name><surname>Paninski</surname><given-names>L</given-names></name>, <name><surname>Pillow</surname><given-names>J</given-names></name>, <name><surname>Lewi</surname><given-names>J</given-names></name>. <chapter-title>Statistical models for neural encoding, decoding, and optimal stimulus design</chapter-title> In: <name><surname>Paul Cisek</surname><given-names>TD</given-names></name>, <name><surname>Kalaska</surname><given-names>JF</given-names></name>, editors. <source>Computational Neuroscience: Theoretical Insights into Brain Function</source>. vol. 165 of Progress in Brain Research. <publisher-name>Elsevier</publisher-name>; <year>2007</year> p. <fpage>493</fpage>&#8211;<lpage>507</lpage>.</mixed-citation></ref><ref id="pcbi.1005763.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Pillow</surname><given-names>JW</given-names></name>, <name><surname>Shlens</surname><given-names>J</given-names></name>, <name><surname>Paninski</surname><given-names>L</given-names></name>, <name><surname>Sher</surname><given-names>A</given-names></name>, <name><surname>Litke</surname><given-names>AM</given-names></name>, <name><surname>Chichilnisky</surname><given-names>EJ</given-names></name>, <etal>et al</etal>
<article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>&#8211;<lpage>999</lpage>. <pub-id pub-id-type="doi">10.1038/nature07140</pub-id>
<?supplied-pmid 18650810?><pub-id pub-id-type="pmid">18650810</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Bialek</surname><given-names>W</given-names></name>, <name><surname>Rieke</surname><given-names>F</given-names></name>, <name><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name>, <name><surname>Warland</surname><given-names>D</given-names></name>. <article-title>Reading a neural code</article-title>. <source>Science</source>. <year>1991</year>;<volume>252</volume>(<issue>5014</issue>):<fpage>1854</fpage>&#8211;<lpage>1857</lpage>. <pub-id pub-id-type="doi">10.1126/science.2063199</pub-id>
<?supplied-pmid 2063199?><pub-id pub-id-type="pmid">2063199</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Botella-Soler</surname><given-names>V</given-names></name>, <name><surname>Simmons</surname><given-names>KD</given-names></name>, <name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>. <article-title>High Accuracy Decoding of Dynamical Motion from a Large Retinal Population</article-title>. <source>PLOS Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>7</issue>):<fpage>1</fpage>&#8211;<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004304</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref006"><label>6</label><mixed-citation publication-type="other">Botella-Soler V, Deny S, Marre O, Tka&#269;ik G. Nonlinear decoding of a complex movie from the mammalian retina. Arxiv preprint arXiv:160503373. 2016;.</mixed-citation></ref><ref id="pcbi.1005763.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Schneidman</surname><given-names>E</given-names></name>. <article-title>Towards the design principles of neural population codes</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2016</year>;<volume>37</volume>:<fpage>133</fpage>&#8212;<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2016.03.001</pub-id>
<?supplied-pmid 27016639?><pub-id pub-id-type="pmid">27016639</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Schneidman</surname><given-names>E</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <name><surname>Segev</surname><given-names>R</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source>. <year>2006</year>;<volume>440</volume>(<issue>7087</issue>):<fpage>1007</fpage>&#8211;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1038/nature04701</pub-id>
<?supplied-pmid 16625187?><pub-id pub-id-type="pmid">16625187</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Shlens</surname><given-names>J</given-names></name>, <name><surname>Field</surname><given-names>GD</given-names></name>, <name><surname>Gauthier</surname><given-names>JL</given-names></name>, <name><surname>Grivich</surname><given-names>MI</given-names></name>, <name><surname>Petrusca</surname><given-names>D</given-names></name>, <name><surname>Sher</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>The Structure of Multi-Neuron Firing Patterns in Primate Retina</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year>;<volume>26</volume>(<issue>32</issue>):<fpage>8254</fpage>&#8211;<lpage>8266</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1282-06.2006</pub-id>
<?supplied-pmid 16899720?><pub-id pub-id-type="pmid">16899720</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref010"><label>10</label><mixed-citation publication-type="other">Tka&#269;ik G, Schneidman E, Berry MJ II, Bialek W. Ising models for networks of real neurons. Arxiv preprint q-bio/0611072. 2006;.</mixed-citation></ref><ref id="pcbi.1005763.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Amodei</surname><given-names>D</given-names></name>, <name><surname>Schneidman</surname><given-names>E</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>. <article-title>Searching for Collective Behavior in a Large Network of Sensory Neurons</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>1</issue>):<fpage>e1003408</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003408</pub-id>
<?supplied-pmid 24391485?><pub-id pub-id-type="pmid">24391485</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Ohiorhenuan</surname><given-names>IE</given-names></name>, <name><surname>Mechler</surname><given-names>F</given-names></name>, <name><surname>Purpura</surname><given-names>KP</given-names></name>, <name><surname>Schmid</surname><given-names>AM</given-names></name>, <name><surname>Hu</surname><given-names>Q</given-names></name>, <name><surname>Victor</surname><given-names>JD</given-names></name>. <article-title>Sparse coding and high-order correlations in fine-scale cortical networks</article-title>. <source>Nature</source>. <year>2010</year>;<volume>466</volume>(<issue>7306</issue>):<fpage>617</fpage>&#8211;<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1038/nature09178</pub-id>
<?supplied-pmid 20601940?><pub-id pub-id-type="pmid">20601940</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Okun</surname><given-names>M</given-names></name>, <name><surname>Yger</surname><given-names>P</given-names></name>, <name><surname>Marguet</surname><given-names>SL</given-names></name>, <name><surname>Gerard-Mercier</surname><given-names>F</given-names></name>, <name><surname>Benucci</surname><given-names>A</given-names></name>, <name><surname>Katzner</surname><given-names>S</given-names></name>, <etal>et al</etal>
<article-title>Population Rate Dynamics and Multineuron Firing Patterns in Sensory Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>48</issue>):<fpage>17108</fpage>&#8211;<lpage>17119</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1831-12.2012</pub-id>
<?supplied-pmid 23197704?><pub-id pub-id-type="pmid">23197704</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Amodei</surname><given-names>D</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <article-title>The simplest maximum entropy model for collective behavior in a neural network</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>. <year>2013</year>;<volume>2013</volume>(<issue>03</issue>):<fpage>P03011</fpage>
<pub-id pub-id-type="doi">10.1088/1742-5468/2013/03/P03011</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Shimazaki</surname><given-names>H</given-names></name>, <name><surname>Sadeghi</surname><given-names>K</given-names></name>, <name><surname>Ishikawa</surname><given-names>T</given-names></name>, <name><surname>Ikegaya</surname><given-names>Y</given-names></name>, <name><surname>Toyoizumi</surname><given-names>T</given-names></name>. <article-title>Simultaneous silence organizes structured higher-order interactions in neural populations</article-title>. <source>Scientific reports</source>. <year>2015</year>;<volume>5</volume> (<issue>9821</issue>).</mixed-citation></ref><ref id="pcbi.1005763.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Okun</surname><given-names>M</given-names></name>, <name><surname>Steinmetz</surname><given-names>NA</given-names></name>, <name><surname>Cossell</surname><given-names>L</given-names></name>, <name><surname>Iacaruso</surname><given-names>MF</given-names></name>, <name><surname>Ko</surname><given-names>H</given-names></name>, <name><surname>Bartho</surname><given-names>P</given-names></name>, <etal>et al</etal>
<article-title>Diverse coupling of neurons to populations in sensory cortex</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>511</fpage>&#8211;<lpage>515</lpage>. <pub-id pub-id-type="doi">10.1038/nature14273</pub-id>
<?supplied-pmid 25849776?><pub-id pub-id-type="pmid">25849776</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref017"><label>17</label><mixed-citation publication-type="journal">
<name><surname>O&#8217;Donnell</surname><given-names>C</given-names></name>, <name><surname>Goncalves</surname><given-names>JT</given-names></name>, <name><surname>Whiteley</surname><given-names>N</given-names></name>, <name><surname>Portera-Cailliau</surname><given-names>C</given-names></name>, <name><surname>Sejnowski</surname><given-names>TJ</given-names></name>. <source>The population tracking model: A simple, scalable statistical model for neural population data</source>. <year>2017</year>; p. <fpage>50</fpage>&#8211;<lpage>93</lpage>.</mixed-citation></ref><ref id="pcbi.1005763.ref018"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Gardella</surname><given-names>C</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Mora</surname><given-names>T</given-names></name>. <article-title>A Tractable Method for Describing Complex Couplings between Neurons and Population Rate</article-title>. <source>eneuro</source>. <year>2016</year>;<volume>3</volume>(<issue>4</issue>). <pub-id pub-id-type="doi">10.1523/ENEURO.0160-15.2016</pub-id>
<?supplied-pmid 27570827?><pub-id pub-id-type="pmid">27570827</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Amodei</surname><given-names>D</given-names></name>, <name><surname>Palmer</surname><given-names>SE</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <etal>et al</etal>
<article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>37</issue>):<fpage>11508</fpage>&#8211;<lpage>11513</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1514188112</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Beggs</surname><given-names>JM</given-names></name>, <name><surname>Plenz</surname><given-names>D</given-names></name>. <article-title>Neuronal avalanches in neocortical circuits</article-title>. <source>The Journal of neuroscience</source>. <year>2003</year>;<volume>23</volume>(<issue>35</issue>):<fpage>11167</fpage>&#8211;<lpage>11177</lpage>. <?supplied-pmid 14657176?><pub-id pub-id-type="pmid">14657176</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref021"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Levina</surname><given-names>A</given-names></name>, <name><surname>Herrmann</surname><given-names>JM</given-names></name>, <name><surname>Geisel</surname><given-names>T</given-names></name>. <article-title>Dynamical synapses causing self-organized criticality in neural networks</article-title>. <source>Nature physics</source>. <year>2007</year>;<volume>3</volume>(<issue>12</issue>):<fpage>857</fpage>&#8211;<lpage>860</lpage>. <pub-id pub-id-type="doi">10.1038/nphys758</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref022"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Deny</surname><given-names>S</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>. <article-title>Dynamical criticality in the collective activity of a population of retinal neurons</article-title>. <source>Phys Rev Lett</source>. <year>2015</year>;<volume>114</volume>(<issue>7</issue>):<fpage>078105</fpage>
<pub-id pub-id-type="doi">10.1103/PhysRevLett.114.078105</pub-id>
<?supplied-pmid 25763977?><pub-id pub-id-type="pmid">25763977</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <article-title>Are Biological Systems Poised at Criticality?</article-title>
<source>Journal of Statistical Physics</source>. <year>2011</year>;<volume>144</volume>(<issue>2</issue>):<fpage>268</fpage>&#8211;<lpage>302</lpage>. <pub-id pub-id-type="doi">10.1007/s10955-011-0229-4</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref024"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Ganmor</surname><given-names>E</given-names></name>, <name><surname>Segev</surname><given-names>R</given-names></name>, <name><surname>Schneidman</surname><given-names>E</given-names></name>. <article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>23</issue>):<fpage>9679</fpage>&#8211;<lpage>9684</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1019641108</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref025"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Mastromatteo</surname><given-names>I</given-names></name>, <name><surname>Marsili</surname><given-names>M</given-names></name>. <article-title>On the criticality of inferred models</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>. <year>2011</year>;(<issue>10</issue>):<fpage>P10012</fpage>
<pub-id pub-id-type="doi">10.1088/1742-5468/2011/10/P10012</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Macke</surname><given-names>JH</given-names></name>, <name><surname>Opper</surname><given-names>M</given-names></name>, <name><surname>Bethge</surname><given-names>M</given-names></name>. <article-title>Common Input Explains Higher-Order Correlations and Entropy in a Simple Model of Neural Population Activity</article-title>. <source>Phys Rev Lett</source>. <year>2011</year>;<volume>106</volume>:<fpage>208102</fpage>
<pub-id pub-id-type="doi">10.1103/PhysRevLett.106.208102</pub-id>
<?supplied-pmid 21668265?><pub-id pub-id-type="pmid">21668265</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Castellana</surname><given-names>M</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <article-title>Inverse Spin Glass and Related Maximum Entropy Problems</article-title>. <source>Phys Rev Lett</source>. <year>2014</year>;<volume>113</volume>:<fpage>117204</fpage>
<pub-id pub-id-type="doi">10.1103/PhysRevLett.113.117204</pub-id>
<?supplied-pmid 25260004?><pub-id pub-id-type="pmid">25260004</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref028"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Schwab</surname><given-names>DJ</given-names></name>, <name><surname>Nemenman</surname><given-names>I</given-names></name>, <name><surname>Mehta</surname><given-names>P</given-names></name>. <article-title>Zipf&#8217;s Law and Criticality in Multivariate Data without Fine-Tuning</article-title>. <source>Phys Rev Lett</source>. <year>2014</year>;<volume>113</volume>(<issue>6</issue>):<fpage>068102</fpage>
<pub-id pub-id-type="doi">10.1103/PhysRevLett.113.068102</pub-id>
<?supplied-pmid 25148352?><pub-id pub-id-type="pmid">25148352</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref029"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Aitchison</surname><given-names>L</given-names></name>, <name><surname>Corradi</surname><given-names>N</given-names></name>, <name><surname>Latham</surname><given-names>PE</given-names></name>. <article-title>Zipf&#8217;s Law Arises Naturally When There Are Underlying, Unobserved Variables</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>12</issue>):<fpage>1</fpage>&#8211;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005110</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref030"><label>30</label><mixed-citation publication-type="other">Nonnenmacher M, Behrens C, Berens P, Bethge M, Macke JH. Signatures of criticality arise in simple neural population models with correlations. arXiv preprint arXiv:160300097. 2016;.</mixed-citation></ref><ref id="pcbi.1005763.ref031"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Stephens</surname><given-names>GJ</given-names></name>, <name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <article-title>Statistical Thermodynamics of Natural Images</article-title>. <source>Physical Review Letters</source>. <year>2013</year>;<volume>110</volume>(<issue>1</issue>):<fpage>018701</fpage>
<pub-id pub-id-type="doi">10.1103/PhysRevLett.110.018701</pub-id>
<?supplied-pmid 23383852?><pub-id pub-id-type="pmid">23383852</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref032"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Saremi</surname><given-names>S</given-names></name>, <name><surname>Sejnowski</surname><given-names>TJ</given-names></name>. <article-title>Hierarchical model of natural images and the origin of scale invariance</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year>;<volume>110</volume>(<issue>8</issue>):<fpage>3071</fpage>&#8211;<lpage>3076</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1222618110</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref033"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Mora</surname><given-names>T</given-names></name>, <name><surname>Walczak</surname><given-names>AM</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>, <name><surname>Callan</surname><given-names>CG</given-names></name>. <article-title>Maximum entropy models for antibody diversity</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2010</year>;<volume>107</volume>(<issue>12</issue>):<fpage>5405</fpage>&#8211;<lpage>5410</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1001705107</pub-id>
<?supplied-pmid 20212159?><pub-id pub-id-type="pmid">20212159</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref034"><label>34</label><mixed-citation publication-type="journal">
<name><surname>K&#246;ster</surname><given-names>U</given-names></name>, <name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name>, <name><surname>Gray</surname><given-names>CM</given-names></name>, <name><surname>Olshausen</surname><given-names>BA</given-names></name>. <article-title>Modeling Higher-Order Correlations within Cortical Microcolumns</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>7</issue>):<fpage>e1003684</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003684</pub-id>
<?supplied-pmid 24991969?><pub-id pub-id-type="pmid">24991969</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref035"><label>35</label><mixed-citation publication-type="book">
<name><surname>Stanley</surname><given-names>HE</given-names></name>. <chapter-title>Introduction to Phase Transitions and Critical Phenomena</chapter-title>
<source>International series of monographs on physics</source>. <publisher-name>Oxford University Press</publisher-name>; <year>1971</year>.</mixed-citation></ref><ref id="pcbi.1005763.ref036"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Ramsay</surname><given-names>JO</given-names></name>. <article-title>Estimating smooth monotone functions</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>1998</year>;<volume>60</volume>(<issue>2</issue>):<fpage>365</fpage>&#8211;<lpage>375</lpage>. <pub-id pub-id-type="doi">10.1111/1467-9868.00130</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Amodei</surname><given-names>D</given-names></name>, <name><surname>Schneidman</surname><given-names>E</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>. <source>Multi-electrode array recording from salamander retinal ganglion cells</source>; <year>2017</year> Available from: <pub-id pub-id-type="doi">10.15479/AT:ISTA:61</pub-id>.</mixed-citation></ref><ref id="pcbi.1005763.ref038"><label>38</label><mixed-citation publication-type="other">Tieleman T. Training Restricted Boltzmann Machines Using Approximations to the Likelihood Gradient. In: Proceedings of the 25th International Conference on Machine Learning. ICML&#8217;08. New York, NY, USA: ACM; 2008. p. 1064&#8211;1071.</mixed-citation></ref><ref id="pcbi.1005763.ref039"><label>39</label><mixed-citation publication-type="journal">
<name><surname>Amari</surname><given-names>S</given-names></name>, <name><surname>Nakahara</surname><given-names>H</given-names></name>, <name><surname>Wu</surname><given-names>S</given-names></name>, <name><surname>Sakai</surname><given-names>Y</given-names></name>. <article-title>Synchronous Firing and Higher-Order Interactions in Neuron Pool</article-title>. <source>Neural Computation</source>. <year>2003</year>;<volume>15</volume>(<issue>1</issue>):<fpage>127</fpage>&#8211;<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1162/089976603321043720</pub-id>
<?supplied-pmid 12590822?><pub-id pub-id-type="pmid">12590822</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref040"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Landau</surname><given-names>DP</given-names></name>. <article-title>Efficient, Multiple-Range Random Walk Algorithm to Calculate the Density of States</article-title>. <source>Phys Rev Lett</source>. <year>2001</year>;<volume>86</volume>:<fpage>2050</fpage>&#8211;<lpage>2053</lpage>. <pub-id pub-id-type="doi">10.1103/PhysRevLett.86.2050</pub-id>
<?supplied-pmid 11289852?><pub-id pub-id-type="pmid">11289852</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref041"><label>41</label><mixed-citation publication-type="book">
<name><surname>Widder</surname><given-names>DV</given-names></name>. <source>The Laplace transform. Princeton mathematical series</source>. <publisher-name>Princeton university press</publisher-name>; <year>1946</year>.</mixed-citation></ref><ref id="pcbi.1005763.ref042"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Marsili</surname><given-names>M</given-names></name>, <name><surname>Mastromatteo</surname><given-names>I</given-names></name>, <name><surname>Roudi</surname><given-names>Y</given-names></name>. <article-title>On sampling and modeling complex systems</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>. <year>2013</year>;(<issue>09</issue>):<fpage>P09003</fpage>
<pub-id pub-id-type="doi">10.1088/1742-5468/2013/09/P09003</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Prentice</surname><given-names>JS</given-names></name>, <name><surname>Marre</surname><given-names>O</given-names></name>, <name><surname>Ioffe</surname><given-names>ML</given-names></name>, <name><surname>Loback</surname><given-names>AR</given-names></name>, <name><surname>Tka&#269;ik</surname><given-names>G</given-names></name>, <name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>. <article-title>Error-Robust Modes of the Retinal Population Code</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>11</issue>):<fpage>1</fpage>&#8211;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005148</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref044"><label>44</label><mixed-citation publication-type="other">Loback AR, Prentice JS, Ioffe ML, Berry MJ II. Noise-robust modes of the retinal population code geometrically correspond with &#8220;ridges&#8221;. Arxiv preprint arXiv:161006886. 2016;.</mixed-citation></ref><ref id="pcbi.1005763.ref045"><label>45</label><mixed-citation publication-type="journal">
<name><surname>Shimazaki</surname><given-names>H</given-names></name>, <name><surname>Amari</surname><given-names>S</given-names></name>, <name><surname>Brown</surname><given-names>EN</given-names></name>, <name><surname>Gr&#252;n</surname><given-names>S</given-names></name>. <article-title>State-Space Analysis of Time-Varying Higher-Order Spike Correlation for Multiple Neural Spike Train Data</article-title>. <source>PLOS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>3</issue>):<fpage>1</fpage>&#8211;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002385</pub-id><pub-id pub-id-type="pmid">22629235</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref046"><label>46</label><mixed-citation publication-type="journal">
<name><surname>Donner</surname><given-names>C</given-names></name>, <name><surname>Obermayer</surname><given-names>K</given-names></name>, <name><surname>Shimazaki</surname><given-names>H</given-names></name>. <article-title>Approximate Inference for Time-Varying Interactions and Macroscopic Dynamics of Neural Populations</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>1</fpage>&#8211;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005309</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref047"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Yu</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>H</given-names></name>, <name><surname>Nakahara</surname><given-names>H</given-names></name>, <name><surname>Santos</surname><given-names>GS</given-names></name>, <name><surname>Nikoli&#263;</surname><given-names>D</given-names></name>, <name><surname>Plenz</surname><given-names>D</given-names></name>. <article-title>Higher-Order Interactions Characterized in Cortical Activity</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>48</issue>):<fpage>17514</fpage>&#8211;<lpage>17526</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3127-11.2011</pub-id>
<?supplied-pmid 22131413?><pub-id pub-id-type="pmid">22131413</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref048"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Montani</surname><given-names>F</given-names></name>, <name><surname>Phoka</surname><given-names>E</given-names></name>, <name><surname>Portesi</surname><given-names>M</given-names></name>, <name><surname>Schultz</surname><given-names>SR</given-names></name>. <article-title>Statistical modelling of higher-order correlations in pools of neural activity</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2013</year>;<volume>392</volume>(<issue>14</issue>):<fpage>3066</fpage>&#8211;<lpage>3086</lpage>. <pub-id pub-id-type="doi">10.1016/j.physa.2013.03.012</pub-id>.</mixed-citation></ref><ref id="pcbi.1005763.ref049"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Montangie</surname><given-names>L</given-names></name>, <name><surname>Montani</surname><given-names>F</given-names></name>. <article-title>Quantifying higher-order correlations in a neuronal pool</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2015</year>;<volume>421</volume>:<fpage>388</fpage>&#8211;<lpage>400</lpage>. <pub-id pub-id-type="doi">10.1016/j.physa.2014.11.046</pub-id>.</mixed-citation></ref><ref id="pcbi.1005763.ref050"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Montangie</surname><given-names>L</given-names></name>, <name><surname>Montani</surname><given-names>F</given-names></name>. <article-title>Higher-order correlations in common input shapes the output spiking activity of a neural population</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2017</year>;<volume>471</volume>:<fpage>845</fpage>&#8211;<lpage>861</lpage>. <pub-id pub-id-type="doi">10.1016/j.physa.2016.12.002</pub-id>.</mixed-citation></ref><ref id="pcbi.1005763.ref051"><label>51</label><mixed-citation publication-type="other">Lyu S, Simoncelli EP. Reducing statistical dependencies in natural signals using radial Gaussianization. In: Advances in neural information processing systems; 2009. p. 1009&#8211;1016.</mixed-citation></ref><ref id="pcbi.1005763.ref052"><label>52</label><mixed-citation publication-type="book">
<name><surname>Gerhard</surname><given-names>HE</given-names></name>, <name><surname>Theis</surname><given-names>L</given-names></name>, <name><surname>Bethge</surname><given-names>M</given-names></name>. <chapter-title>Modeling Natural Image Statistics</chapter-title> In: <source>Biologically-inspired Computer Vision&#8212;Fundamentals and Applications</source>. <publisher-name>Wiley VCH</publisher-name>; <year>2015</year>.</mixed-citation></ref><ref id="pcbi.1005763.ref053"><label>53</label><mixed-citation publication-type="journal">
<name><surname>Hanel</surname><given-names>R</given-names></name>, <name><surname>Thurner</surname><given-names>S</given-names></name>. <article-title>When do generalized entropies apply? How phase space volume determines entropy</article-title>. <source>EPL (Europhysics Letters)</source>. <year>2011</year>;<volume>96</volume>(<issue>5</issue>):<fpage>50003</fpage>
<pub-id pub-id-type="doi">10.1209/0295-5075/96/50003</pub-id></mixed-citation></ref><ref id="pcbi.1005763.ref054"><label>54</label><mixed-citation publication-type="book">
<name><surname>Frenkel</surname><given-names>D</given-names></name>, <name><surname>Smit</surname><given-names>B</given-names></name>. <source>Understanding Molecular Simulation: From Algorithms to Applications</source>. <publisher-name>Academic Press</publisher-name>; <year>1996</year>.</mixed-citation></ref><ref id="pcbi.1005763.ref055"><label>55</label><mixed-citation publication-type="journal">
<name><surname>Neal</surname><given-names>RM</given-names></name>. <article-title>Annealed importance sampling</article-title>. <source>Statistics and Computing</source>. <year>2001</year>;<volume>11</volume>(<issue>2</issue>):<fpage>125</fpage>&#8211;<lpage>139</lpage>. <pub-id pub-id-type="doi">10.1023/A:1008923215028</pub-id></mixed-citation></ref></ref-list></back></article>